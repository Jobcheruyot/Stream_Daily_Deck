# -*- coding: utf-8 -*-
"""DailyDeck.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rh7shhxVseHFAjfEBpoIJfwbUEYDM_ZK

##*Install required tools

#*Import Libraries
"""

# Consolidated imports (auto-cleaned)
from IPython.display import display
from google.colab import files
import ipywidgets as widgets
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import textwrap
from datetime import datetime
from datetime import time
from datetime import timedelta
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# === STYLING & DEFAULT SETTINGS ===
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("coolwarm")
pd.set_option('display.float_format', '{:,.2f}'.format)

"""#*Upload Data set"""

df = pd.read_csv(
    "/content/DAILY_POS_TRN_ITEMS_2025-10-31.csv",
    on_bad_lines='skip',   # skips rows that break structure
    low_memory=False
)

df.head()

"""#*Convert Dates to Datetime"""

# Convert TRN_DATE and ZED_DATE to datetime
df['TRN_DATE'] = pd.to_datetime(df['TRN_DATE'], errors='coerce')
df['ZED_DATE'] = pd.to_datetime(df['ZED_DATE'], errors='coerce')

# View the changes
df[['TRN_DATE', 'ZED_DATE']].head()

"""#*Clean  Numeric Columns for analysis"""

# List of numeric columns for analysis
numeric_cols = [
    'QTY',
    'CP_PRE_VAT',
    'SP_PRE_VAT',
    'COST_PRE_VAT',
    'NET_SALES',
    'VAT_AMT'
]

# Convert to numeric (handles commas, empty cells, and errors)
for col in numeric_cols:
    df[col] = (
        df[col]
        .astype(str)                 # ensure string for cleaning
        .str.replace(',', '', regex=False)  # remove thousand separators
        .str.strip()                 # remove spaces
    )
    df[col] = pd.to_numeric(df[col], errors='coerce')  # convert to float

# ‚úÖ Confirm conversion
df[numeric_cols].dtypes

df.head()

"""#SALES

##Global sales Overview
"""

import plotly.graph_objects as go
import pandas as pd

# Ensure NET_SALES is numeric
df['NET_SALES'] = pd.to_numeric(df['NET_SALES'], errors='coerce').fillna(0)

# ==========================
# Aggregate NET_SALES globally
# ==========================
global_sales = (
    df.groupby('SALES_CHANNEL_L1', as_index=False)['NET_SALES']
      .sum()
      .sort_values('NET_SALES', ascending=False)
)

# Convert sales to millions
global_sales['NET_SALES_M'] = global_sales['NET_SALES'] / 1_000_000

# Calculate percentages
global_sales['PCT'] = (global_sales['NET_SALES'] / global_sales['NET_SALES'].sum()) * 100

# Legend labels: include % and actual sales in millions
legend_labels = [
    f"{row['SALES_CHANNEL_L1']} ({row['PCT']:.1f}% | {row['NET_SALES_M']:.1f} M)"
    for _, row in global_sales.iterrows()
]

values = global_sales['NET_SALES_M']  # use millions for the pie chart scale

# ==========================
# Donut Chart
# ==========================
colors = ['#1f77b4','#ff7f0e','#2ca02c','#d62728','#9467bd','#8c564b',
          '#e377c2','#7f7f7f','#bcbd22','#17becf']

fig = go.Figure(data=[go.Pie(
    labels=legend_labels,
    values=values,
    hole=0.65,
    text=[f"{p:.1f}%" for p in global_sales['PCT']],
    textinfo='text',
    textposition='inside',
    insidetextorientation='auto',
    sort=True,
    marker=dict(colors=colors, line=dict(color='white', width=1)),
    hovertemplate='<b>%{label}</b><br>KSh %{value:,.2f} M<extra></extra>'
)])

fig.update_layout(
    title="<b>SALES CHANNEL TYPE ‚Äî Global Overview</b>",
    title_x=0.42,
    margin=dict(l=40, r=40, t=70, b=40),
    legend_title_text="Sales Channels (% | KSh Millions)",
    showlegend=True,
    height=600
)

fig.show()

"""##Global Net Sales Distribution by Sales Channel"""

import plotly.graph_objects as go
import pandas as pd

# --- Ensure numeric ---
df['NET_SALES'] = pd.to_numeric(df['NET_SALES'], errors='coerce').fillna(0)

# --- Aggregate NET_SALES by SALES_CHANNEL_L2 ---
channel2_sales = (
    df.groupby('SALES_CHANNEL_L2', as_index=False)['NET_SALES']
      .sum()
      .sort_values('NET_SALES', ascending=False)
)

# --- Convert sales to millions ---
channel2_sales['NET_SALES_M'] = channel2_sales['NET_SALES'] / 1_000_000

# --- Calculate percentages ---
channel2_sales['PCT'] = (
    channel2_sales['NET_SALES'] / channel2_sales['NET_SALES'].sum() * 100
)

# --- Total Global Sales (Millions) ---
total_sales_m = channel2_sales['NET_SALES_M'].sum()

# --- Legend labels with % and millions ---
legend_labels = [
    f"{row['SALES_CHANNEL_L2']} ({row['PCT']:.1f}% | {row['NET_SALES_M']:.1f} M)"
    for _, row in channel2_sales.iterrows()
]

values = channel2_sales['NET_SALES_M']

# --- Define colors ---
colors = [
    '#1f77b4','#ff7f0e','#2ca02c','#d62728',
    '#9467bd','#8c564b','#e377c2','#7f7f7f',
    '#bcbd22','#17becf'
]

# --- Build donut chart ---
fig = go.Figure(data=[go.Pie(
    labels=legend_labels,
    values=values,
    hole=0.65,
    text=[f"{p:.1f}%" for p in channel2_sales['PCT']],
    textinfo='text',
    textposition='inside',
    insidetextorientation='auto',
    marker=dict(colors=colors, line=dict(color='white', width=1)),
    hovertemplate='<b>%{label}</b><br>KSh %{value:,.2f} M<extra></extra>'
)])

fig.update_layout(
    title="<b>Global Net Sales Distribution by Sales Mode (SALES_CHANNEL_L2)</b>",
    title_x=0.43,
    margin=dict(l=50, r=50, t=80, b=40),
    legend_title_text=f"Sales Mode (% | KSh Millions)\n<b>Total Global Sales: {total_sales_m:,.1f} M</b>",
    showlegend=True,
    height=620
)

fig.show()

"""##Global Net Sales Distribution by SHIFT"""

# --- Make sure NET_SALES is numeric ---
df['NET_SALES'] = pd.to_numeric(df['NET_SALES'], errors='coerce').fillna(0)

# --- Aggregate by SHIFT ---
shift_sales = (df.groupby('SHIFT', as_index=False)['NET_SALES']
                  .sum()
                  .sort_values('NET_SALES', ascending=False))

# --- Calculate percentages ---
shift_sales['PCT'] = (shift_sales['NET_SALES'] / shift_sales['NET_SALES'].sum()) * 100
legend_labels = [f"{row['SHIFT']} ({row['PCT']:.1f}%)" for _, row in shift_sales.iterrows()]

# --- Color palette ---
colors = ['#1f77b4','#ff7f0e','#2ca02c','#d62728','#9467bd','#8c564b']

# --- Donut Chart ---
fig = go.Figure(data=[go.Pie(
    labels=legend_labels,
    values=shift_sales['NET_SALES'],
    hole=0.65,
    text=[f"{p:.1f}%" for p in shift_sales['PCT']],
    textinfo='text',
    textposition='inside',
    insidetextorientation='auto',
    marker=dict(colors=colors, line=dict(color='white', width=1)),
    hovertemplate='<b>%{label}</b><br>KSh %{value:,.0f}<extra></extra>'
)])

fig.update_layout(
    title="<b>Global Net Sales Distribution by SHIFT</b>",
    title_x=0.5,
    margin=dict(l=20, r=20, t=60, b=20),
    legend_title_text="SHIFT (with %)",
    showlegend=True,
    height=550
)

fig.show()

"""##Night vs Day Shift Sales Ratio ‚Äî Stores with Night Shifts"""

# ================================
# Night vs Day Ratio ‚Äî Full Code
# Ranked DESC by Night %, #1 on top
# ================================


# --- 1) Build pivot_df (Night/Day % per store) ---

# Ensure numeric
df['NET_SALES'] = pd.to_numeric(df['NET_SALES'], errors='coerce').fillna(0)

# Stores that have at least one NIGHT shift
stores_with_night = df[df['SHIFT'].str.upper().str.contains('NIGHT', na=False)]['STORE_NAME'].unique()

# Keep only those stores
df_nd = df[df['STORE_NAME'].isin(stores_with_night)].copy()

# Bucket: Night vs Day
df_nd['Shift_Bucket'] = np.where(
    df_nd['SHIFT'].str.upper().str.contains('NIGHT', na=False), 'Night', 'Day'
)

# Aggregate sales per store & bucket
ratio_df = (df_nd.groupby(['STORE_NAME','Shift_Bucket'], as_index=False)['NET_SALES']
            .sum())

# Percent of store total
store_totals = ratio_df.groupby('STORE_NAME')['NET_SALES'].transform('sum')
ratio_df['PCT'] = 100 * ratio_df['NET_SALES'] / store_totals

# Pivot to wide table: columns = Day/Night (missing -> 0)
pivot_df = ratio_df.pivot(index='STORE_NAME', columns='Shift_Bucket', values='PCT').fillna(0)

# --- 2) Plot: Ranked top-down by Night %, numbered, with Day % labels ---

# Sort DESC by Night so highest Night% is #1 at the top
pivot_sorted = pivot_df.sort_values(by='Night', ascending=False)

# Build numbered y labels AFTER sorting
stores = pivot_sorted.index.tolist()
numbered_labels = [f"{i+1}. {store}" for i, store in enumerate(stores)]

night_vals = pivot_sorted['Night'].values
day_vals   = pivot_sorted['Day'].values

# Auto height (compact)
rows = len(numbered_labels)
per_row = 22
height = int(np.clip(120 + per_row * rows, 360, 1200))

# Auto x-range (avoid wasted space)
max_night = float(np.max(night_vals)) if len(night_vals) else 0.0
pad = 3.0
xmax = max(10.0, min(100.0, max_night + pad))

fig = go.Figure()

# Night bars (red)
fig.add_trace(go.Bar(
    x=night_vals,
    y=numbered_labels,
    orientation='h',
    name='Night',
    marker_color='#d62728',
    text=[f"{v:.1f}%" for v in night_vals],
    textposition='inside',
    insidetextanchor='middle',
    textfont=dict(color='white', size=10),
    hovertemplate='Night: %{x:.1f}%<extra></extra>'
))

# Day % annotation to the right of each bar
for i, (night, day) in enumerate(zip(night_vals, day_vals)):
    # place just past end of night bar; keep inside frame
    x_pos = min(night + 0.8, xmax - 0.5 if xmax > 1 else 0.95)
    if night < 3:
        x_pos = min(night + 1.2, xmax - 0.5)
    fig.add_annotation(
        x=x_pos,
        y=numbered_labels[i],
        text=f"{day:.1f}% Day",
        showarrow=False,
        xanchor='left',
        yanchor='middle',
        font=dict(size=10, color='#111')
    )

# Dummy Day trace so legend shows both
fig.add_trace(go.Bar(
    x=[0], y=[None], orientation='h',
    name='Day', marker_color='#1f77b4', showlegend=True
))

# Layout
fig.update_layout(
    barmode='stack',
    title='Night vs Day Shift Sales Ratio ‚Äî Stores with Night Shifts',
    xaxis_title='% of Store Sales',
    yaxis_title='Store Name',
    xaxis=dict(range=[0, xmax], tickmode='linear', dtick=10, gridcolor='rgba(0,0,0,0.15)'),
    height=height,
    margin=dict(l=130, r=20, t=60, b=30),
    legend_title_text='Shift',
    bargap=0.20,
    uniformtext_minsize=9,
    uniformtext_mode='hide'
)

fig.update_yaxes(automargin=True)
fig.show()

"""##Global Day vs Night Sales ‚Äî Only Stores with NIGHT Shift"""

# Ensure numeric
df['NET_SALES'] = pd.to_numeric(df['NET_SALES'], errors='coerce').fillna(0)

# Stores that have at least one NIGHT shift
stores_with_night = df[df['SHIFT'].str.upper().str.contains('NIGHT', na=False)]['STORE_NAME'].unique()

# Keep only those stores
df_nd = df[df['STORE_NAME'].isin(stores_with_night)].copy()

# Day/Night bucket
df_nd['Shift_Bucket'] = np.where(
    df_nd['SHIFT'].str.upper().str.contains('NIGHT', na=False), 'Night', 'Day'
)

# GLOBAL totals across the selected stores
global_nd = (df_nd.groupby('Shift_Bucket', as_index=False)['NET_SALES']
               .sum()
               .sort_values('NET_SALES', ascending=False))

# Percentages (for legend + labels)
global_nd['PCT'] = 100 * global_nd['NET_SALES'] / global_nd['NET_SALES'].sum()
legend_labels = [f"{b} ({p:.1f}%)" for b, p in zip(global_nd['Shift_Bucket'], global_nd['PCT'])]

# Donut chart (single global view)
fig = go.Figure(go.Pie(
    labels=legend_labels,
    values=global_nd['NET_SALES'],
    hole=0.65,
    text=[f"{p:.1f}%" for p in global_nd['PCT']],
    textinfo='text',
    textposition='inside',
    insidetextorientation='auto',
    marker=dict(colors=['#1f77b4', '#d62728'], line=dict(color='white', width=1)),
    hovertemplate='<b>%{label}</b><br>KSh %{value:,.0f}<extra></extra>',
    sort=False
))

fig.update_layout(
    title="<b>Global Day vs Night Sales ‚Äî Only Stores with NIGHT Shifts</b>",
    title_x=0.5,
    showlegend=True,
    legend_title_text="Shift (with %)",
    margin=dict(l=20, r=20, t=60, b=20),
    height=520
)

fig.show()

"""##2nd-Highest Channel Share"""

# ==========================================
# LOLLIPOP RANK: 2nd-Highest Channel Share
# ==========================================
# - Uses existing `df` (already loaded in your notebook)
# - Ranks stores by % share of their 2nd-largest SALES_CHANNEL_L1
# - Shows two clean visuals: Top 30 and Bottom 30 (one visual each)
# ==========================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import textwrap

# ---------- SETTINGS ----------
TOP_N = 30
WRAP_WIDTH = 22            # wrap long store names for readability
POINT_COLOR = "#1f77b4"    # blue for points
STEM_COLOR = "#9aa0a6"     # subtle gray for stems
LABEL_COLOR = "#111111"    # dark label text
GRID_ALPHA = 0.25
# -----------------------------

# --- 0) Safety checks & copy ---
required_cols = {"STORE_NAME", "SALES_CHANNEL_L1", "NET_SALES"}
missing = required_cols - set(df.columns)
if missing:
    raise ValueError(f"Missing required columns in df: {missing}")

data = df.copy()

# --- 1) Clean numeric NET_SALES ---
data["NET_SALES"] = (
    data["NET_SALES"]
      .astype(str)
      .str.replace(",", "", regex=False)
      .str.strip()
)
data["NET_SALES"] = pd.to_numeric(data["NET_SALES"], errors="coerce").fillna(0)

# --- 2) Aggregate to store x channel, compute within-store % shares ---
store_chan = data.groupby(["STORE_NAME", "SALES_CHANNEL_L1"], as_index=False)["NET_SALES"].sum()
store_tot = store_chan.groupby("STORE_NAME")["NET_SALES"].transform("sum")
store_chan["PCT"] = 100 * store_chan["NET_SALES"] / store_tot

# --- 3) Rank channels within each store and pick the 2nd highest ---
store_chan = store_chan.sort_values(["STORE_NAME", "PCT"], ascending=[True, False])
store_chan["RANK"] = store_chan.groupby("STORE_NAME").cumcount() + 1

second_tbl = (
    store_chan[store_chan["RANK"] == 2][["STORE_NAME", "SALES_CHANNEL_L1", "PCT"]]
      .rename(columns={"SALES_CHANNEL_L1": "SECOND_CHANNEL", "PCT": "SECOND_PCT"})
)

# If a store has only one channel, set SECOND_PCT = 0
all_stores = store_chan["STORE_NAME"].drop_duplicates()
missing_stores = set(all_stores) - set(second_tbl["STORE_NAME"])
if missing_stores:
    second_tbl = pd.concat(
        [
            second_tbl,
            pd.DataFrame(
                {
                    "STORE_NAME": list(missing_stores),
                    "SECOND_CHANNEL": ["(None)"] * len(missing_stores),
                    "SECOND_PCT": [0.0] * len(missing_stores),
                }
            ),
        ],
        ignore_index=True,
    )

# --- 4) Sort to get Top 30 and Bottom 30 by SECOND_PCT ---
second_tbl_sorted = second_tbl.sort_values("SECOND_PCT", ascending=False)
top_30 = second_tbl_sorted.head(TOP_N).copy()
bottom_30 = second_tbl_sorted.tail(TOP_N).copy().sort_values("SECOND_PCT", ascending=True)

def wrap_text(s, width=WRAP_WIDTH):
    return "\n".join(textwrap.wrap(str(s), width=width)) if isinstance(s, str) else s

def plot_lollipop(df_subset, title, figsize=(12, 12)):
    df_plot = df_subset.copy()
    df_plot["LABEL"] = df_plot["STORE_NAME"].apply(lambda s: wrap_text(s, width=WRAP_WIDTH))
    y = np.arange(len(df_plot))

    fig, ax = plt.subplots(figsize=figsize)

    # stems
    ax.hlines(y=y, xmin=0, xmax=df_plot["SECOND_PCT"], colors=STEM_COLOR, linewidth=2.5)

    # points
    ax.scatter(
        df_plot["SECOND_PCT"], y,
        color=POINT_COLOR, s=70, zorder=3, edgecolor="white", linewidth=0.8
    )

    # value labels
    for i, (val, ch) in enumerate(zip(df_plot["SECOND_PCT"], df_plot["SECOND_CHANNEL"])):
        if val > 0:
            ax.text(
                val + 0.8, i, f"{val:.1f}%  ({ch})",
                va="center", ha="left", fontsize=10.5,
                color=LABEL_COLOR, weight="bold"
            )

    # formatting
    ax.set_yticks(y)
    ax.set_yticklabels(df_plot["LABEL"], fontsize=10.5)
    ax.set_xlim(0, max(5, df_plot["SECOND_PCT"].max() + 5))
    ax.set_xlabel("2nd-Highest Channel Share (% of Store NET_SALES)", fontsize=11)
    ax.set_title(title, fontsize=14, weight="bold", pad=12)
    ax.grid(axis="x", linestyle="--", alpha=GRID_ALPHA)
    plt.box(False)
    plt.tight_layout()
    plt.show()

# --- 5) Plot Top 30 (one visual) ---
plot_lollipop(
    top_30,
    title="Top 30 Stores by 2nd-Highest Channel Share (SALES_CHANNEL_L1)"
)

# --- 6) Plot Bottom 30 (one visual) ---
plot_lollipop(
    bottom_30,
    title="Bottom 30 Stores by 2nd-Highest Channel Share (SALES_CHANNEL_L1)"
)

"""##Bottom 30 ‚Äî 2nd Highest Channel"""

# ============================================
# 2nd-HIGHEST CHANNEL SHARE ‚Äî TOP & BOTTOM 30
# ============================================


# ---------- SETTINGS ----------
TOP_N = 30
WRAP_WIDTH = 22
POINT_COLOR = "#1f77b4"   # blue
STEM_COLOR = "#9aa0a6"    # gray
LABEL_COLOR = "#111111"
GRID_ALPHA = 0.25

# ---------- DATA PREP ----------
# Assuming df is already loaded (and has columns below)
# Columns needed: STORE_NAME, SALES_CHANNEL_L1, NET_SALES

need = {"STORE_NAME", "SALES_CHANNEL_L1", "NET_SALES"}
missing = need - set(df.columns)
if missing:
    raise ValueError(f"Missing required columns: {missing}")

# Clean NET_SALES
df["NET_SALES"] = (
    df["NET_SALES"].astype(str).str.replace(",", "", regex=False).str.strip()
)
df["NET_SALES"] = pd.to_numeric(df["NET_SALES"], errors="coerce").fillna(0)

# Group by store + channel
store_chan = df.groupby(["STORE_NAME", "SALES_CHANNEL_L1"], as_index=False)["NET_SALES"].sum()
store_tot = store_chan.groupby("STORE_NAME")["NET_SALES"].transform("sum")
store_chan["PCT"] = 100 * store_chan["NET_SALES"] / store_tot

# Rank within each store
store_chan = store_chan.sort_values(["STORE_NAME", "PCT"], ascending=[True, False])
store_chan["RANK"] = store_chan.groupby("STORE_NAME").cumcount() + 1

# Extract Top (rank 1) and Second (rank 2)
top_tbl = store_chan[store_chan["RANK"] == 1][
    ["STORE_NAME", "SALES_CHANNEL_L1", "PCT"]
].rename(columns={"SALES_CHANNEL_L1": "TOP_CHANNEL", "PCT": "TOP_PCT"})

second_tbl = store_chan[store_chan["RANK"] == 2][
    ["STORE_NAME", "SALES_CHANNEL_L1", "PCT"]
].rename(columns={"SALES_CHANNEL_L1": "SECOND_CHANNEL", "PCT": "SECOND_PCT"})

# Merge into a single ranking table
ranking = pd.merge(top_tbl, second_tbl, on="STORE_NAME", how="left").fillna({
    "SECOND_CHANNEL": "(None)",
    "SECOND_PCT": 0
})

# Sort descending by SECOND_PCT
ranking_sorted = ranking.sort_values("SECOND_PCT", ascending=False)

# ============================================
# PLOT TOP 30
# ============================================
TOP_30 = ranking_sorted.head(TOP_N)

def wrap_text(s, width=WRAP_WIDTH):
    return "\n".join(textwrap.wrap(str(s), width=width)) if isinstance(s, str) else s

df_plot = TOP_30.copy()
df_plot["LABEL"] = df_plot["STORE_NAME"].apply(lambda s: wrap_text(s, width=WRAP_WIDTH))
y = np.arange(len(df_plot))

plt.figure(figsize=(12, 12))
plt.hlines(y, xmin=0, xmax=df_plot["SECOND_PCT"], colors=STEM_COLOR, linewidth=2.5)
plt.scatter(df_plot["SECOND_PCT"], y, color=POINT_COLOR, s=70, edgecolor="white", linewidth=0.8)

for i, (sec_val, top_val, sec_ch, top_ch) in enumerate(
    zip(df_plot["SECOND_PCT"], df_plot["TOP_PCT"], df_plot["SECOND_CHANNEL"], df_plot["TOP_CHANNEL"])
):
    if sec_val > 0:
        plt.text(sec_val + 0.8, i,
                 f"{sec_val:.1f}% ({sec_ch}) | Top: {top_val:.1f}% ({top_ch})",
                 va="center", ha="left", fontsize=10.5, color=LABEL_COLOR, weight="bold")
    else:
        plt.text(0.8, i,
                 f"Top: {top_val:.1f}% ({top_ch})",
                 va="center", ha="left", fontsize=10.5, color=LABEL_COLOR, weight="bold")

plt.yticks(y, df_plot["LABEL"], fontsize=10.5)
plt.xlim(0, max(5, df_plot["SECOND_PCT"].max() + 5))
plt.xlabel("2nd-Highest Channel Share (% of Store NET_SALES)", fontsize=11)
plt.title("Top 30 Stores by 2nd-Highest Channel Share (SALES_CHANNEL_L1)",
          fontsize=14, weight="bold", pad=12)
plt.grid(axis="x", linestyle="--", alpha=GRID_ALPHA)
plt.box(False)
plt.tight_layout()
plt.show()


# ============================================
# PLOT BOTTOM 30
# ============================================
BOTTOM_30 = ranking_sorted.tail(TOP_N).sort_values("SECOND_PCT", ascending=True)

df_plot = BOTTOM_30.copy()
df_plot["LABEL"] = df_plot["STORE_NAME"].apply(lambda s: wrap_text(s, width=WRAP_WIDTH))
y = np.arange(len(df_plot))

plt.figure(figsize=(12, 12))
plt.hlines(y, xmin=0, xmax=df_plot["SECOND_PCT"], colors=STEM_COLOR, linewidth=2.5)
plt.scatter(df_plot["SECOND_PCT"], y, color=POINT_COLOR, s=70, edgecolor="white", linewidth=0.8)

for i, (sec_val, top_val, sec_ch, top_ch) in enumerate(
    zip(df_plot["SECOND_PCT"], df_plot["TOP_PCT"], df_plot["SECOND_CHANNEL"], df_plot["TOP_CHANNEL"])
):
    if sec_val > 0:
        plt.text(sec_val + 0.8, i,
                 f"{sec_val:.1f}% ({sec_ch}) | Top: {top_val:.1f}% ({top_ch})",
                 va="center", ha="left", fontsize=10.5, color=LABEL_COLOR, weight="bold")
    else:
        plt.text(0.8, i,
                 f"Top: {top_val:.1f}% ({top_ch})",
                 va="center", ha="left", fontsize=10.5, color=LABEL_COLOR, weight="bold")

plt.yticks(y, df_plot["LABEL"], fontsize=10.5)
plt.xlim(0, max(5, df_plot["SECOND_PCT"].max() + 5))
plt.xlabel("2nd-Highest Channel Share (% of Store NET_SALES)", fontsize=11)
plt.title("Bottom 30 Stores by 2nd-Highest Channel Share (SALES_CHANNEL_L1)",
          fontsize=14, weight="bold", pad=12)
plt.grid(axis="x", linestyle="--", alpha=GRID_ALPHA)
plt.box(False)
plt.tight_layout()
plt.show()

"""###*Sales Workings_1

###*Sales Workings_2
"""

# Ensure both columns are numeric
df['NET_SALES'] = pd.to_numeric(df['NET_SALES'], errors='coerce').fillna(0)
df['VAT_AMT'] = pd.to_numeric(df['VAT_AMT'], errors='coerce').fillna(0)

# Create the new column
df['GROSS_SALES'] = df['NET_SALES'] + df['VAT_AMT']

# Preview the result
df[['NET_SALES', 'VAT_AMT', 'GROSS_SALES']].head()

"""###*Sales Workings_3"""

# Ensure the columns are strings to avoid issues with numbers or NaN
df['STORE_CODE'] = df['STORE_CODE'].astype(str).fillna('')
df['TILL'] = df['TILL'].astype(str).fillna('')
df['SESSION'] = df['SESSION'].astype(str).fillna('')
df['RCT'] = df['RCT'].astype(str).fillna('')

# Create the CUST_CODE column by concatenating the fields
df['CUST_CODE'] = (
    df['STORE_CODE'].str.strip() + '-' +
    df['TILL'].str.strip() + '-' +
    df['SESSION'].str.strip() + '-' +
    df['RCT'].str.strip()
)

# Optional: Verify uniqueness (this is useful for counting receipts)
unique_receipts = df['CUST_CODE'].nunique()

print(f"‚úÖ New column 'CUST_CODE' created successfully.")
print(f"üßæ Total unique receipts across all stores: {unique_receipts:,}")

# Preview the result
df[['STORE_CODE', 'TILL', 'SESSION', 'RCT', 'CUST_CODE']].head()

"""###*Sales Workings_4"""

# Ensure numeric types
df['NET_SALES'] = pd.to_numeric(df['NET_SALES'], errors='coerce').fillna(0)
df['VAT_AMT'] = pd.to_numeric(df['VAT_AMT'], errors='coerce').fillna(0)

# Create GROSS_SALES column if missing
if 'GROSS_SALES' not in df.columns:
    df['GROSS_SALES'] = df['NET_SALES'] + df['VAT_AMT']

# Aggregate totals per store
sales_summary = (df.groupby('STORE_NAME', as_index=False)[['NET_SALES','GROSS_SALES']]
                   .sum()
                   .sort_values('GROSS_SALES', ascending=False))

# Compute contribution to total gross sales
total_gross = sales_summary['GROSS_SALES'].sum()
sales_summary['% Contribution'] = (sales_summary['GROSS_SALES'] / total_gross * 100).round(2)

# Format numbers with commas
sales_summary['NET_SALES'] = sales_summary['NET_SALES'].map('{:,.0f}'.format)
sales_summary['GROSS_SALES'] = sales_summary['GROSS_SALES'].map('{:,.0f}'.format)
sales_summary['% Contribution'] = sales_summary['% Contribution'].map('{:,.2f}%'.format)

# Add numbering
sales_summary.reset_index(drop=True, inplace=True)
sales_summary.index += 1

# Display the full DataFrame without truncation
pd.set_option('display.max_rows', None)        # Show all rows
pd.set_option('display.max_columns', None)     # Show all columns
pd.set_option('display.width', 1500)           # Prevent wrapping
pd.set_option('display.colheader_justify', 'left')

sales_summary

"""###*Sales Workings_5"""

# @title
# ‚úÖ Count unique CUST_CODEs per store (customer/receipt counts)
customer_counts = df.groupby('STORE_NAME')['CUST_CODE'].nunique().reset_index()
customer_counts.rename(columns={'CUST_CODE': 'Customer Numbers'}, inplace=True)

# üßæ Merge the counts into the existing sales_summary table
sales_summary = sales_summary.merge(customer_counts, on='STORE_NAME', how='left')

# Optional: Reorder columns so Customer Numbers appears after GROSS_SALES
cols = ['NET_SALES', 'GROSS_SALES', '% Contribution', 'Customer Numbers']
sales_summary = sales_summary[['STORE_NAME'] + cols] if 'STORE_NAME' in sales_summary.columns else sales_summary

# Re-number the index
sales_summary.reset_index(drop=True, inplace=True)
sales_summary.index += 1

# Ensure Customer Numbers is formatted with commas for readability
sales_summary['Customer Numbers'] = sales_summary['Customer Numbers'].map('{:,}'.format)

# Display the full table without truncation
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1500)
pd.set_option('display.colheader_justify', 'left')

sales_summary

"""##Stores Sales Summary"""

import pandas as pd
import numpy as np

# ==============================================
# üßπ 1) Clean + Coerce numeric columns
# ==============================================
def _to_num(s):
    """Convert strings like '1,234' or '12.5%' to numeric safely"""
    return pd.to_numeric(
        pd.Series(s).astype(str).str.replace(r'[,%]', '', regex=True),
        errors='coerce'
    ).fillna(0)

# Work on a copy of your sales_summary DataFrame
ss = sales_summary.copy()

# Remove any existing total rows
ss = ss[~ss['STORE_NAME'].astype(str).str.strip().str.upper().isin(['TOTAL', 'TOTALS'])].copy()

# Coerce key numeric columns
for col in ['NET_SALES', 'GROSS_SALES', 'Customer Numbers']:
    ss[col] = _to_num(ss[col])

# ==============================================
# üßÆ 2) Aggregate by STORE_NAME
# ==============================================
agg = (
    ss.groupby('STORE_NAME', as_index=False)
      .agg(
          NET_SALES=('NET_SALES','sum'),
          GROSS_SALES=('GROSS_SALES','sum'),
          Customer_Numbers=('Customer Numbers','sum')
      )
)

# Compute Gross_BV (weighted average)
agg['Gross_BV'] = np.where(
    agg['Customer_Numbers'] > 0,
    agg['GROSS_SALES'] / agg['Customer_Numbers'],
    0.0
)

# Compute % Contribution
total_net = agg['NET_SALES'].sum()
agg['% Contribution'] = np.where(
    total_net > 0,
    (agg['NET_SALES'] / total_net) * 100,
    0.0
)

# ==============================================
# üìä 3) Sort by NET_SALES descending
# ==============================================
agg = agg.sort_values('NET_SALES', ascending=False).reset_index(drop=True)

# ==============================================
# üßæ 4) Create ONE Total Row
# ==============================================
tot_row = pd.DataFrame({
    'STORE_NAME': ['Total'],
    'NET_SALES': [agg['NET_SALES'].sum()],
    'GROSS_SALES': [agg['GROSS_SALES'].sum()],
    'Customer_Numbers': [agg['Customer_Numbers'].sum()],
    'Gross_BV': [(
        agg['GROSS_SALES'].sum() / agg['Customer_Numbers'].sum()
        if agg['Customer_Numbers'].sum() > 0 else 0.0
    )],
    '% Contribution': [100.0]
})

# Combine total + store rows
final = pd.concat([tot_row, agg], ignore_index=True)

# ==============================================
# üé® 5) Format for Display
# ==============================================
disp = final.copy()
disp['NET_SALES'] = disp['NET_SALES'].map('{:,.0f}'.format)
disp['GROSS_SALES'] = disp['GROSS_SALES'].map('{:,.0f}'.format)
disp['Customer_Numbers'] = disp['Customer_Numbers'].map('{:,.0f}'.format)
disp['Gross_BV'] = disp['Gross_BV'].map('{:,.2f}'.format)
disp['% Contribution'] = disp['% Contribution'].map('{:,.2f}'.format)

# Reorder columns neatly
disp = disp[['STORE_NAME', 'NET_SALES', 'GROSS_SALES', '% Contribution', 'Customer_Numbers', 'Gross_BV']]

# ==============================================
# üî¢ 6) Add Numbering (Skip Total)
# ==============================================
disp.insert(0, '#', '', allow_duplicates=True)
if len(disp) > 1:
    disp.loc[disp.index[1]:, '#'] = list(range(1, len(disp)))

# ==============================================
# üñ•Ô∏è 7) Display Setup
# ==============================================
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1500)
pd.set_option('display.colheader_justify', 'left')

disp

"""#OPERATIONS

##Customer Traffic-Storewise
"""

# -----------------------------
# 1) Clean basics / ensure cols
# -----------------------------
df['TRN_DATE'] = pd.to_datetime(df['TRN_DATE'], errors='coerce')

# Build CUST_CODE robustly (skip if you already have it)
for col in ['STORE_CODE','TILL','SESSION','RCT']:
    df[col] = df[col].astype(str).fillna('').str.strip()

df['CUST_CODE'] = df['STORE_CODE'] + '-' + df['TILL'] + '-' + df['SESSION'] + '-' + df['RCT']

# ------------------------------------------
# 2) DEDUPE: one slot per receipt per store/day
#    Use the earliest time for that receipt-day
# ------------------------------------------
df['TRN_DATE_ONLY'] = df['TRN_DATE'].dt.date

# Earliest timestamp per (STORE_NAME, DATE, CUST_CODE)
first_touch = (
    df.dropna(subset=['TRN_DATE'])
      .groupby(['STORE_NAME','TRN_DATE_ONLY','CUST_CODE'], as_index=False)['TRN_DATE']
      .min()
)

# Bucket to 30-min slot using the *earliest* time
first_touch['TIME_INTERVAL'] = first_touch['TRN_DATE'].dt.floor('30T')
first_touch['TIME_ONLY'] = first_touch['TIME_INTERVAL'].dt.time

# ------------------------------------------------
# 3) Build complete 30-min grid (00:00 ‚Üí 23:30)
# ------------------------------------------------
start_time = pd.Timestamp("00:00:00")
intervals = [(start_time + timedelta(minutes=30*i)).time() for i in range(48)]
col_labels = [f"{t.hour:02d}:{t.minute:02d}" for t in intervals]

# ------------------------------------------
# 4) Count unique receipts per store/interval
#    (now guaranteed non-overlapping by time)
# ------------------------------------------
counts = (
    first_touch.groupby(['STORE_NAME','TIME_ONLY'])['CUST_CODE']
               .nunique()
               .reset_index(name='RECEIPT_COUNT')
)

# Pivot to wide
heatmap = counts.pivot(index='STORE_NAME', columns='TIME_ONLY', values='RECEIPT_COUNT').fillna(0)

# Ensure all interval columns exist and ordered
for t in intervals:
    if t not in heatmap.columns:
        heatmap[t] = 0
heatmap = heatmap[intervals]

# Sort stores by total receipts (busiest first)
heatmap['TOTAL'] = heatmap.sum(axis=1)
heatmap = heatmap.sort_values('TOTAL', ascending=False)
totals = heatmap['TOTAL'].astype(int).copy()
heatmap_matrix = heatmap.drop(columns=['TOTAL'])

# ------------------------------------------
# 5) Heatmap with zero in gray + text overlay
# ------------------------------------------
colorscale = [
    [0.0,  '#E6E6E6'],  # zeros
    [0.001,'#FFFFCC'],
    [0.25, '#FED976'],
    [0.50, '#FEB24C'],
    [0.75, '#FD8D3C'],
    [1.0,  '#E31A1C']
]

z = heatmap_matrix.values
zmax = float(z.max()) if z.size else 1.0
if zmax <= 0: zmax = 1.0

fig = px.imshow(
    z,
    x=col_labels,
    y=heatmap_matrix.index,
    text_auto=True,
    aspect='auto',
    color_continuous_scale=colorscale,
    zmin=0, zmax=zmax,
    labels=dict(x="Time Interval (30 min)", y="Store Name", color="Receipts")
)

# >>> Put the Time of Day axis at the TOP <<<
fig.update_xaxes(side='top')

# Show store totals on the left (not part of color scale)
for i, total in enumerate(totals):
    fig.add_annotation(
        x=-0.6, y=i,
        text=f"{total:,}",
        showarrow=False,
        xanchor='right', yanchor='middle',
        font=dict(size=11, color='black')
    )

# Header for totals
fig.add_annotation(
    x=-0.6, y=-1,
    text="<b>TOTAL</b>",
    showarrow=False,
    xanchor='right', yanchor='top',
    font=dict(size=12, color='black')
)

fig.update_layout(
    title="Customer Traffic Heatmap",
    xaxis_title="Time of Day",
    yaxis_title="Store Name",
    height=max(600, 25 * len(heatmap_matrix.index)),
    margin=dict(l=185, r=20, t=85, b=45),  # a bit more top margin to clear the top axis
    coloraxis_colorbar=dict(title="Receipt Count")
)

fig.show()

# ------------------------------------------
# 6) VALIDATION (optional but recommended)
#    Compare deduped totals vs raw unique per store
# ------------------------------------------
# Raw unique (per store/day/receipt)
raw_unique = (
    df.dropna(subset=['TRN_DATE'])
      .groupby(['STORE_NAME','TRN_DATE_ONLY'])['CUST_CODE']
      .nunique()
      .groupby('STORE_NAME').sum()
)

check = pd.DataFrame({
    'Dedup_Heatmap_Total': totals,
    'Raw_Unique_Total': raw_unique
}).fillna(0).astype(int)
check['Match'] = np.where(check['Dedup_Heatmap_Total'] == check['Raw_Unique_Total'], 'OK', 'MISMATCH')

# Example inspection:
# display(check.loc[['OTC']])
# display(check[check['Match'] == 'MISMATCH'])

"""##Active Tills During the day"""

# Ensure both columns are strings
df['TILL'] = df['TILL'].astype(str).fillna('').str.strip()
df['STORE_CODE'] = df['STORE_CODE'].astype(str).fillna('').str.strip()

# Create new column
df['Till_Code'] = df['TILL'] + '-' + df['STORE_CODE']

# --- 1) Prepare Till_Code ---
df['TILL'] = df['TILL'].astype(str).fillna('').str.strip()
df['STORE_CODE'] = df['STORE_CODE'].astype(str).fillna('').str.strip()

if 'Till_Code' not in df.columns:
    df['Till_Code'] = df['TILL'] + '-' + df['STORE_CODE']

# Ensure TRN_DATE is valid datetime
df['TRN_DATE'] = pd.to_datetime(df['TRN_DATE'], errors='coerce')
df = df.dropna(subset=['TRN_DATE'])

# Bucket into 30-min intervals
df['TIME_INTERVAL'] = df['TRN_DATE'].dt.floor('30T')
df['TIME_ONLY'] = df['TIME_INTERVAL'].dt.time

# --- 2) Build time grid ---
start_time = pd.Timestamp("00:00:00")
intervals = [(start_time + timedelta(minutes=30*i)).time() for i in range(48)]
col_labels = [f"{t.hour:02d}:{t.minute:02d}" for t in intervals]

# --- 3) Count unique tills per interval ---
till_counts = (
    df.groupby(['STORE_NAME','TIME_ONLY'])['Till_Code']
      .nunique()
      .reset_index(name='UNIQUE_TILLS')
)

# Pivot to heatmap matrix
heatmap = till_counts.pivot(index='STORE_NAME', columns='TIME_ONLY', values='UNIQUE_TILLS').fillna(0)

# Ensure all intervals exist
for t in intervals:
    if t not in heatmap.columns:
        heatmap[t] = 0
heatmap = heatmap[intervals]

# --- 4) Calculate Max active tills per store ---
heatmap['MAX_TILLS'] = heatmap.max(axis=1).astype(int)
heatmap = heatmap.sort_values('MAX_TILLS', ascending=False)

# Separate max column for display only
max_vals = heatmap['MAX_TILLS'].copy()
mat = heatmap.drop(columns=['MAX_TILLS']).values

# --- 5) Colorscale ---
colorscale = [
    [0.0,  '#E6E6E6'],
    [0.001,'#FFFFCC'],
    [0.25, '#FED976'],
    [0.50, '#FEB24C'],
    [0.75, '#FD8D3C'],
    [1.0,  '#E31A1C']
]
zmax = float(mat.max()) if mat.size else 1.0
if zmax <= 0:
    zmax = 1.0

# --- 6) Heatmap ---
fig = px.imshow(
    mat,
    x=col_labels,
    y=heatmap.index,
    text_auto=True,
    aspect='auto',
    color_continuous_scale=colorscale,
    zmin=0, zmax=zmax,
    labels=dict(x="Time Interval (30 min)", y="Store Name", color="Unique Tills")
)

# üïí Move time labels to TOP
fig.update_xaxes(side='top')

# Max labels on the left
for i, max_till in enumerate(max_vals):
    fig.add_annotation(
        x=-0.6, y=i,
        text=f"{int(max_till):,}",
        showarrow=False,
        xanchor='right',
        yanchor='middle',
        font=dict(size=11, color='black')
    )

# Header for max column
fig.add_annotation(
    x=-0.6, y=-1,
    text="<b>MAX</b>",
    showarrow=False,
    xanchor='right',
    yanchor='top',
    font=dict(size=12, color='black')
)

fig.update_layout(
    title="Peak Active Tills",
    xaxis_title="Time of Day",
    yaxis_title="Store Name",
    height=max(600, 25 * len(heatmap.index)),
    margin=dict(l=180, r=20, t=60, b=40),
    coloraxis_colorbar=dict(title="Unique Tills")
)

fig.show()

"""##Average Customers Served per Till"""

# --- 1) Build time intervals ---
start_time = pd.Timestamp("00:00:00")
intervals = [(start_time + timedelta(minutes=30*i)).time() for i in range(48)]
col_labels = [f"{t.hour:02d}:{t.minute:02d}" for t in intervals]

# --- 2) Customer counts (earliest receipt per till) ---
df['TRN_DATE'] = pd.to_datetime(df['TRN_DATE'], errors='coerce')
df = df.dropna(subset=['TRN_DATE'])

for c in ['STORE_CODE','TILL','SESSION','RCT']:
    df[c] = df[c].astype(str).fillna('').str.strip()

df['CUST_CODE'] = df['STORE_CODE'] + '-' + df['TILL'] + '-' + df['SESSION'] + '-' + df['RCT']
df['TRN_DATE_ONLY'] = df['TRN_DATE'].dt.date

first_touch = (
    df.groupby(['STORE_NAME','TRN_DATE_ONLY','CUST_CODE'], as_index=False)['TRN_DATE'].min()
)
first_touch['TIME_INTERVAL'] = first_touch['TRN_DATE'].dt.floor('30T')
first_touch['TIME_ONLY'] = first_touch['TIME_INTERVAL'].dt.time

customer_counts = (
    first_touch.groupby(['STORE_NAME','TIME_ONLY'])['CUST_CODE']
    .nunique().reset_index(name='CUSTOMERS')
)
cust_pivot = customer_counts.pivot(index='STORE_NAME', columns='TIME_ONLY', values='CUSTOMERS').fillna(0)
for t in intervals:
    if t not in cust_pivot.columns:
        cust_pivot[t] = 0
cust_pivot = cust_pivot[intervals]

# --- 3) Till counts ---
df['TILL'] = df['TILL'].astype(str).fillna('').str.strip()
df['STORE_CODE'] = df['STORE_CODE'].astype(str).fillna('').str.strip()
df['Till_Code'] = df['TILL'] + '-' + df['STORE_CODE']
df['TIME_INTERVAL'] = df['TRN_DATE'].dt.floor('30T')
df['TIME_ONLY'] = df['TIME_INTERVAL'].dt.time

till_counts = (
    df.groupby(['STORE_NAME','TIME_ONLY'])['Till_Code']
    .nunique().reset_index(name='TILLS')
)
till_pivot = till_counts.pivot(index='STORE_NAME', columns='TIME_ONLY', values='TILLS').fillna(0)
for t in intervals:
    if t not in till_pivot.columns:
        till_pivot[t] = 0
till_pivot = till_pivot[intervals]

# --- 4) Calculate and round up Customers per Till ---
ratio_matrix = cust_pivot / till_pivot.replace(0, np.nan)
ratio_matrix = np.ceil(ratio_matrix).fillna(0).astype(int)

ratio_matrix['MAX_RATIO'] = ratio_matrix.max(axis=1)
ratio_matrix = ratio_matrix.sort_values('MAX_RATIO', ascending=False)
max_vals = ratio_matrix['MAX_RATIO']
ratio_data = ratio_matrix.drop(columns=['MAX_RATIO']).values

# --- 5) Plot the heatmap ---
colorscale = [
    [0.0,  '#E6E6E6'],   # 0 = gray
    [0.001,'#e0f3db'],
    [0.25, '#a8ddb5'],
    [0.50, '#43a2ca'],
    [0.75, '#0868ac'],
    [1.0,  '#084081']
]
zmax = float(ratio_data.max()) if ratio_data.size else 1.0
if zmax <= 0:
    zmax = 1.0

fig = px.imshow(
    ratio_data,
    x=col_labels,
    y=ratio_matrix.index,
    text_auto=True,
    aspect='auto',
    color_continuous_scale=colorscale,
    zmin=0,
    zmax=zmax,
    labels=dict(x="Time Interval (30 min)", y="Store Name", color="Customers per Till")
)

# Move time labels to top
fig.update_xaxes(side='top')

# Add max labels on the left
for i, val in enumerate(max_vals):
    fig.add_annotation(
        x=-0.6, y=i,
        text=f"{val}",
        showarrow=False,
        xanchor='right', yanchor='middle',
        font=dict(size=11, color='black')
    )

# Header for max column
fig.add_annotation(
    x=-0.6, y=-1,
    text="<b>MAX</b>",
    showarrow=False,
    xanchor='right', yanchor='top',
    font=dict(size=12, color='black')
)

fig.update_layout(
    title="Customers Served per Till ",
    xaxis_title="Time of Day",
    yaxis_title="Store Name",
    height=max(600, 25 * len(ratio_matrix.index)),
    margin=dict(l=190, r=30, t=60, b=60),
    coloraxis_colorbar=dict(title="Customers / Till")
)

fig.show()

"""##Store Customer Traffic Storewise"""

import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import timedelta

# =========================
# 0) Prepare the data
# =========================
df['TRN_DATE'] = pd.to_datetime(df['TRN_DATE'], errors='coerce')
df = df.dropna(subset=['TRN_DATE']).copy()

# Ensure CUST_CODE exists
for col in ['STORE_CODE','TILL','SESSION','RCT']:
    df[col] = df[col].astype(str).fillna('').str.strip()
if 'CUST_CODE' not in df.columns:
    df['CUST_CODE'] = df['STORE_CODE'] + '-' + df['TILL'] + '-' + df['SESSION'] + '-' + df['RCT']

df['TIME_INTERVAL'] = df['TRN_DATE'].dt.floor('30T')
df['TIME_ONLY'] = df['TIME_INTERVAL'].dt.time

# =========================
# 1) Time grid (30 min)
# =========================
start_time = pd.Timestamp("00:00:00")
intervals = [(start_time + timedelta(minutes=30*i)).time() for i in range(48)]
col_labels = [f"{t.hour:02d}:{t.minute:02d}" for t in intervals]

# =========================
# 2) Helper to build pivot per branch
# =========================
def build_branch_pivot(branch_name: str):
    branch_df = df[df['STORE_NAME'] == branch_name]
    tmp = (
        branch_df.groupby(['DEPARTMENT','TIME_ONLY'])['CUST_CODE']
        .nunique()
        .reset_index(name='Unique_Customers')
    )

    pivot = tmp.pivot(index='DEPARTMENT', columns='TIME_ONLY', values='Unique_Customers').fillna(0)
    for t in intervals:
        if t not in pivot.columns:
            pivot[t] = 0
    pivot = pivot[intervals]

    pivot['TOTAL'] = pivot.sum(axis=1)
    pivot = pivot.sort_values('TOTAL', ascending=False)
    totals = pivot['TOTAL'].astype(int).tolist()
    y_labels = pivot.index.tolist()
    mat = pivot.drop(columns=['TOTAL']).values

    # üëá Total customers for that store (unique CUST_CODE)
    total_customers = branch_df['CUST_CODE'].nunique()

    return mat, y_labels, totals, total_customers

# =========================
# 3) Precompute for all branches
# =========================
branches = sorted(df['STORE_NAME'].dropna().unique().tolist())
branch_data = {}
global_zmax = 1
for b in branches:
    mat, y_labels, totals, total_customers = build_branch_pivot(b)
    branch_data[b] = {'z': mat, 'y': y_labels, 'totals': totals, 'total_customers': total_customers}
    if mat.size:
        global_zmax = max(global_zmax, int(np.max(mat)))

# =========================
# 4) Base figure (initial branch)
# =========================
init_branch = branches[0] if branches else None
z0 = branch_data[init_branch]['z']
y0 = branch_data[init_branch]['y']
totals0 = branch_data[init_branch]['totals']
total_customers0 = branch_data[init_branch]['total_customers']

colorscale = [
    [0.0,  '#E6E6E6'],
    [0.001,'#e0f3db'],
    [0.25, '#a8ddb5'],
    [0.50, '#43a2ca'],
    [0.75, '#0868ac'],
    [1.0,  '#084081']
]

fig = px.imshow(
    z0,
    x=col_labels,
    y=y0,
    text_auto=True,
    aspect='auto',
    color_continuous_scale=colorscale,
    zmin=0,
    zmax=global_zmax,
    labels=dict(x="Time of Day", y="Department", color="Unique Customers")
)

fig.update_xaxes(side='top')

def make_total_annotations(totals, y_labels):
    ann = []
    for i, total in enumerate(totals):
        ann.append(dict(
            x=-0.6, y=i, text=f"{int(total):,}",
            showarrow=False, xanchor='right', yanchor='middle',
            font=dict(size=11, color='black')
        ))
    ann.append(dict(
        x=-0.6, y=-1, text="<b>TOTAL</b>",
        showarrow=False, xanchor='right', yanchor='top',
        font=dict(size=12, color='black')
    ))
    return ann

fig.update_layout(
    title=f"üïí Customer Traffic Patterns ‚Äî {init_branch} | Total Customers: {total_customers0:,}",
    xaxis_title="Time of Day",
    yaxis_title="Department",
    height=max(600, 25 * len(y0)),
    margin=dict(l=180, r=20, t=60, b=40),
    coloraxis_colorbar=dict(title="Customers"),
    annotations=make_total_annotations(totals0, y0)
)

# =========================
# 5) Dropdown
# =========================
buttons = []
for b in branches:
    z = branch_data[b]['z']
    y = branch_data[b]['y']
    totals = branch_data[b]['totals']
    total_customers = branch_data[b]['total_customers']
    new_height = max(600, 25 * len(y))

    buttons.append(dict(
        label=b,
        method='update',
        args=[
            {'z': [z], 'y': [y], 'x': [col_labels]},
            {
                'title': f"üïí Customer Traffic Patterns ‚Äî {b} | Total Customers: {total_customers:,}",
                'annotations': make_total_annotations(totals, y),
                'height': new_height
            }
        ]
    ))

fig.update_layout(
    updatemenus=[dict(
        type='dropdown',
        x=0, xanchor='left',
        y=1.12, yanchor='top',
        buttons=buttons,
        direction='down',
        showactive=True
    )]
)

fig.show()

"""###*Customer Traffic Computation 1"""

# Combine CASHIER and STORE_NAME into a new column
df['CASHIER-COUNT'] = df['CASHIER'].astype(str).str.strip() + '-' + df['STORE_NAME'].astype(str).str.strip()

# Optional: verify the result
df[['CASHIER', 'STORE_NAME', 'CASHIER-COUNT']].head()

"""###*Customer Traffic Computation2"""

# üßº Ensure TRN_DATE is datetime
df['TRN_DATE'] = pd.to_datetime(df['TRN_DATE'], errors='coerce')

# üè™ Create CASHIER-COUNT if not exists
if 'CASHIER-COUNT' not in df.columns:
    df['CASHIER-COUNT'] = df['CASHIER'].astype(str).str.strip() + '-' + df['STORE_NAME'].astype(str).str.strip()

# üñ•Ô∏è Ensure Till_Code exists (combine TILL and STORE_CODE)
if 'Till_Code' not in df.columns:
    df['TILL'] = df['TILL'].astype(str).fillna('').str.strip()
    df['STORE_CODE'] = df['STORE_CODE'].astype(str).fillna('').str.strip()
    df['Till_Code'] = df['TILL'] + '-' + df['STORE_CODE']

# ---------------------------------------------------
# 1) Hours Worked (Max - Min transaction per cashier)
# ---------------------------------------------------
cashier_times = (
    df.groupby('CASHIER-COUNT')['TRN_DATE']
      .agg(['min', 'max'])
      .reset_index()
)
cashier_times['Hours_Worked'] = (cashier_times['max'] - cashier_times['min']).dt.total_seconds() / 3600

# ---------------------------------------------------
# 2) Unique customers served
# ---------------------------------------------------
cust_counts = (
    df.groupby('CASHIER-COUNT')['CUST_CODE']
      .nunique()
      .reset_index()
      .rename(columns={'CUST_CODE': 'Customers_Served'})
)

# ---------------------------------------------------
# 3) Unique tills used per cashier
# ---------------------------------------------------
till_counts = (
    df.groupby('CASHIER-COUNT')['Till_Code']
      .nunique()
      .reset_index()
      .rename(columns={'Till_Code': 'No_of_Tills'})
)

# ---------------------------------------------------
# 4) Merge all into one summary table
# ---------------------------------------------------
summary = cashier_times.merge(cust_counts, on='CASHIER-COUNT', how='left') \
                       .merge(till_counts, on='CASHIER-COUNT', how='left')

summary['Hours_Worked'] = summary['Hours_Worked'].round(2)
summary = summary.sort_values('Hours_Worked', ascending=False).reset_index(drop=True)

# Optional: compute customers per hour
summary['Customers_per_Hour'] = (summary['Customers_Served'] / summary['Hours_Worked']).replace([np.inf, -np.inf], 0).fillna(0).round(1)

# üìä View top 20
print(summary.head(20))
summary.head(60)

"""###*Customer Traffic Computation 3"""

# ‚úÖ Count unique CASHIER-COUNT per STORE_NAME
unique_cashiers_per_store = (
    df.groupby('STORE_NAME')['CASHIER-COUNT']
      .nunique()
      .reset_index(name='Unique_Cashiers')
      .sort_values('Unique_Cashiers', ascending=False)
)

# ü™Ñ Reset index and format numbering to start from 1
unique_cashiers_per_store = unique_cashiers_per_store.reset_index(drop=True)
unique_cashiers_per_store.index = unique_cashiers_per_store.index + 1  # start numbering at 1

# Optional: rename the index to make it clearer
unique_cashiers_per_store.index.name = 'Rank'

# Display result
print(unique_cashiers_per_store)

"""##Customer Traffic-Departmentwise"""

import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import timedelta

# =========================
# 0) Prepare the data
# =========================
df['TRN_DATE'] = pd.to_datetime(df['TRN_DATE'], errors='coerce')
df = df.dropna(subset=['TRN_DATE']).copy()

# Ensure CUST_CODE exists
for col in ['STORE_CODE','TILL','SESSION','RCT']:
    df[col] = df[col].astype(str).fillna('').str.strip()
if 'CUST_CODE' not in df.columns:
    df['CUST_CODE'] = df['STORE_CODE'] + '-' + df['TILL'] + '-' + df['SESSION'] + '-' + df['RCT']

df['TIME_INTERVAL'] = df['TRN_DATE'].dt.floor('30T')
df['TIME_ONLY'] = df['TIME_INTERVAL'].dt.time

# =========================
# 1) Time grid (30 min)
# =========================
start_time = pd.Timestamp("00:00:00")
intervals = [(start_time + timedelta(minutes=30*i)).time() for i in range(48)]
col_labels = [f"{t.hour:02d}:{t.minute:02d}" for t in intervals]

# =========================
# 2) Helper to build pivot per branch
# =========================
def build_branch_pivot(branch_name: str):
    branch_df = df[df['STORE_NAME'] == branch_name]
    tmp = (
        branch_df.groupby(['DEPARTMENT','TIME_ONLY'])['CUST_CODE']
        .nunique()
        .reset_index(name='Unique_Customers')
    )

    pivot = tmp.pivot(index='DEPARTMENT', columns='TIME_ONLY', values='Unique_Customers').fillna(0)
    for t in intervals:
        if t not in pivot.columns:
            pivot[t] = 0
    pivot = pivot[intervals]

    pivot['TOTAL'] = pivot.sum(axis=1)
    pivot = pivot.sort_values('TOTAL', ascending=False)
    totals = pivot['TOTAL'].astype(int).tolist()
    y_labels = pivot.index.tolist()
    mat = pivot.drop(columns=['TOTAL']).values

    # üëá Total customers for that store (unique CUST_CODE)
    total_customers = branch_df['CUST_CODE'].nunique()

    return mat, y_labels, totals, total_customers

# =========================
# 3) Precompute for all branches
# =========================
branches = sorted(df['STORE_NAME'].dropna().unique().tolist())
branch_data = {}
global_zmax = 1
for b in branches:
    mat, y_labels, totals, total_customers = build_branch_pivot(b)
    branch_data[b] = {'z': mat, 'y': y_labels, 'totals': totals, 'total_customers': total_customers}
    if mat.size:
        global_zmax = max(global_zmax, int(np.max(mat)))

# =========================
# 4) Base figure (initial branch)
# =========================
init_branch = branches[0] if branches else None
z0 = branch_data[init_branch]['z']
y0 = branch_data[init_branch]['y']
totals0 = branch_data[init_branch]['totals']
total_customers0 = branch_data[init_branch]['total_customers']

colorscale = [
    [0.0,  '#E6E6E6'],
    [0.001,'#e0f3db'],
    [0.25, '#a8ddb5'],
    [0.50, '#43a2ca'],
    [0.75, '#0868ac'],
    [1.0,  '#084081']
]

fig = px.imshow(
    z0,
    x=col_labels,
    y=y0,
    text_auto=True,
    aspect='auto',
    color_continuous_scale=colorscale,
    zmin=0,
    zmax=global_zmax,
    labels=dict(x="Time of Day", y="Department", color="Unique Customers")
)

fig.update_xaxes(side='top')

def make_total_annotations(totals, y_labels):
    ann = []
    for i, total in enumerate(totals):
        ann.append(dict(
            x=-0.6, y=i, text=f"{int(total):,}",
            showarrow=False, xanchor='right', yanchor='middle',
            font=dict(size=11, color='black')
        ))
    ann.append(dict(
        x=-0.6, y=-1, text="<b>TOTAL</b>",
        showarrow=False, xanchor='right', yanchor='top',
        font=dict(size=12, color='black')
    ))
    return ann

fig.update_layout(
    title=f"üïí Customer Traffic Patterns ‚Äî {init_branch} | Total Customers: {total_customers0:,}",
    xaxis_title="Time of Day",
    yaxis_title="Department",
    height=max(600, 25 * len(y0)),
    margin=dict(l=180, r=20, t=60, b=40),
    coloraxis_colorbar=dict(title="Customers"),
    annotations=make_total_annotations(totals0, y0)
)

# =========================
# 5) Dropdown
# =========================
buttons = []
for b in branches:
    z = branch_data[b]['z']
    y = branch_data[b]['y']
    totals = branch_data[b]['totals']
    total_customers = branch_data[b]['total_customers']
    new_height = max(600, 25 * len(y))

    buttons.append(dict(
        label=b,
        method='update',
        args=[
            {'z': [z], 'y': [y], 'x': [col_labels]},
            {
                'title': f"üïí Customer Traffic Patterns ‚Äî {b} | Total Customers: {total_customers:,}",
                'annotations': make_total_annotations(totals, y),
                'height': new_height
            }
        ]
    ))

fig.update_layout(
    updatemenus=[dict(
        type='dropdown',
        x=0, xanchor='left',
        y=1.12, yanchor='top',
        buttons=buttons,
        direction='down',
        showactive=True
    )]
)

fig.show()

"""##Cashiers Perfomance"""

import pandas as pd
import numpy as np
import plotly.express as px
from datetime import timedelta

# =====================================
# 1Ô∏è‚É£ Prepare Data
# =====================================
df['TRN_DATE'] = pd.to_datetime(df['TRN_DATE'], errors='coerce')
df = df.dropna(subset=['TRN_DATE']).copy()

# Ensure identifiers
for c in ['STORE_CODE', 'TILL', 'SESSION', 'RCT', 'CASHIER', 'ITEM_CODE']:
    if c not in df.columns:
        raise ValueError(f"Missing column {c} in dataset")
    df[c] = df[c].astype(str).fillna('').str.strip()

# Unique receipt
df['CUST_CODE'] = df['STORE_CODE'] + '-' + df['TILL'] + '-' + df['SESSION'] + '-' + df['RCT']

# Create unique cashier per store
df['CASHIER-COUNT'] = df['STORE_NAME'] + '-' + df['CASHIER']

# =====================================
# 2Ô∏è‚É£ Receipt-level duration (min) and item count
# =====================================
receipt_duration = (
    df.groupby(['STORE_NAME', 'CUST_CODE'], as_index=False)
      .agg(Start_Time=('TRN_DATE', 'min'),
           End_Time=('TRN_DATE', 'max'))
)
receipt_duration['Duration_Sec'] = (receipt_duration['End_Time'] - receipt_duration['Start_Time']).dt.total_seconds()
receipt_duration['Duration_Sec'] = receipt_duration['Duration_Sec'].fillna(0)

receipt_items = (
    df.groupby(['STORE_NAME', 'CUST_CODE'], as_index=False)['ITEM_CODE']
      .nunique()
      .rename(columns={'ITEM_CODE': 'Unique_Items'})
)

receipt_stats = pd.merge(receipt_duration, receipt_items, on=['STORE_NAME', 'CUST_CODE'], how='left')

# =====================================
# 3Ô∏è‚É£ Store-level summary
# =====================================
store_summary = (
    receipt_stats.groupby('STORE_NAME', as_index=False)
      .agg(
          Total_Customers=('CUST_CODE', 'nunique'),
          Avg_Time_per_Customer_Min=('Duration_Sec', lambda s: s.mean() / 60),
          Avg_Items_per_Receipt=('Unique_Items', 'mean')
      )
)
store_summary['Avg_Time_per_Customer_Min'] = store_summary['Avg_Time_per_Customer_Min'].round(1)
store_summary['Avg_Items_per_Receipt'] = store_summary['Avg_Items_per_Receipt'].round(1)
store_summary = store_summary.sort_values('Avg_Time_per_Customer_Min', ascending=True).reset_index(drop=True)
store_summary.index = np.arange(1, len(store_summary) + 1)
store_summary.index.name = '#'
store_summary['Total_Customers'] = store_summary['Total_Customers'].map('{:,.0f}'.format)

# =====================================
# 4Ô∏è‚É£ Cashier summary (duration + customer count)
# =====================================
cashier_durations = (
    df.merge(receipt_stats[['STORE_NAME', 'CUST_CODE', 'Duration_Sec']], on=['STORE_NAME','CUST_CODE'], how='left')
      .groupby(['STORE_NAME', 'CASHIER-COUNT'], as_index=False)
      .agg(
          Avg_Duration_Sec=('Duration_Sec', 'mean'),
          Customers_Served=('CUST_CODE', 'nunique')
      )
)
cashier_durations['Avg_Serve_Min'] = (cashier_durations['Avg_Duration_Sec'] / 60.0).round(1)

# =====================================
# 5Ô∏è‚É£ Dropdown chart per branch
# =====================================
branches = sorted(store_summary['STORE_NAME'].unique().tolist())
branch_data = {b: cashier_durations[cashier_durations['STORE_NAME']==b].sort_values('Avg_Serve_Min')
               for b in branches}

init_branch = branches[0]
df_branch = branch_data[init_branch]

# Text column combining time and customer count
df_branch['Label_Text'] = df_branch['Avg_Serve_Min'].astype(str) + ' min (' + df_branch['Customers_Served'].astype(str) + ' customers)'

fig = px.bar(
    df_branch,
    x='Avg_Serve_Min',
    y='CASHIER-COUNT',
    orientation='h',
    text='Label_Text',
    color='Avg_Serve_Min',
    color_continuous_scale='Blues',
    title=f"üïí Avg Serving Time per Cashier ‚Äî {init_branch}",
    labels={'Avg_Serve_Min': 'Avg Time per Customer (min)', 'CASHIER-COUNT': 'Cashier'}
)

fig.update_traces(textposition='outside', textfont=dict(size=10))
fig.update_layout(
    xaxis_title="Average Serving Time (minutes)",
    yaxis_title="Cashier",
    coloraxis_showscale=False,
    height=max(500, 25 * len(df_branch))
)

# =====================================
# 6Ô∏è‚É£ Dropdown for branches
# =====================================
buttons = []
for branch in branches:
    dfb = branch_data[branch].copy()
    dfb['Label_Text'] = dfb['Avg_Serve_Min'].astype(str) + ' min (' + dfb['Customers_Served'].astype(str) + ' customers)'
    buttons.append(dict(
        label=branch,
        method='update',
        args=[{
            'x': [dfb['Avg_Serve_Min']],
            'y': [dfb['CASHIER-COUNT']],
            'text': [dfb['Label_Text']],
            'marker': {'color': dfb['Avg_Serve_Min'], 'colorscale': 'Blues'}
        }, {
            'title': f"üïí Avg Serving Time per Cashier ‚Äî {branch}",
            'height': max(500, 25 * len(dfb))
        }]
    ))

fig.update_layout(
    updatemenus=[dict(
        type='dropdown',
        x=0, xanchor='left',
        y=1.15, yanchor='top',
        buttons=buttons,
        showactive=True
    )]
)

fig.show()

# =====================================
# 7Ô∏è‚É£ Display Final Table
# =====================================
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1500)
display(store_summary)

import pandas as pd
import numpy as np
import plotly.express as px
from datetime import timedelta

# =======================================================
# 1) Prepare and validate data
# =======================================================
df['TRN_DATE'] = pd.to_datetime(df['TRN_DATE'], errors='coerce')
df = df.dropna(subset=['TRN_DATE']).copy()

for c in ['STORE_NAME', 'Till_Code', 'CUST_CODE']:
    if c not in df.columns:
        raise ValueError(f"Missing required column: {c}")
    df[c] = df[c].astype(str).str.strip()

# 30-minute buckets
df['TIME_SLOT'] = df['TRN_DATE'].dt.floor('30T')
df['TIME_ONLY'] = df['TIME_SLOT'].dt.time

# =======================================================
# 2) Aggregate: unique receipts per till every 30 mins
# =======================================================
till_activity = (
    df.groupby(['STORE_NAME', 'Till_Code', 'TIME_ONLY'], as_index=False)
      .agg(Receipts=('CUST_CODE', 'nunique'))
)

# =======================================================
# 3) Build full 30-minute time grid (00:00 ‚Üí 23:30)
# =======================================================
start_time = pd.Timestamp("00:00:00")
intervals = [(start_time + timedelta(minutes=30*i)).time() for i in range(48)]

grid = (
    df[['STORE_NAME','Till_Code']]
      .drop_duplicates()
      .assign(key=1)
      .merge(pd.DataFrame({'TIME_ONLY': intervals, 'key':1}), on='key')
      .drop('key', axis=1)
)

activity_full = (
    grid.merge(till_activity, on=['STORE_NAME','Till_Code','TIME_ONLY'], how='left')
        .fillna({'Receipts':0})
)

# =======================================================
# 4) Summary metrics (+ numbering)
# =======================================================
branch_summary = (
    till_activity.groupby('STORE_NAME', as_index=False)
        .agg(
            Total_Transactions=('Receipts','sum'),
            Avg_Per_Till=('Receipts','mean'),
            Max_Per_Till=('Receipts','max')
        )
)

# Busiest till per store
busiest = (
    till_activity.groupby(['STORE_NAME','Till_Code'], as_index=False)['Receipts'].sum()
    .sort_values(['STORE_NAME','Receipts'], ascending=[True,False])
    .groupby('STORE_NAME').first().reset_index()
    .rename(columns={'Receipts':'Total_Receipts','Till_Code':'Busiest_Till'})
)

branch_summary = (
    branch_summary.merge(busiest, on='STORE_NAME', how='left')
                  .sort_values('Total_Transactions', ascending=False)
                  .reset_index(drop=True)
)

# Number rows 1..N
branch_summary.insert(0, '#', range(1, len(branch_summary)+1))

# (Optional) nice formatting for display
branch_summary_display = branch_summary.copy()
for col in ['Total_Transactions','Total_Receipts']:
    branch_summary_display[col] = branch_summary_display[col].map('{:,.0f}'.format)
branch_summary_display['Avg_Per_Till'] = branch_summary_display['Avg_Per_Till'].round(2)

# =======================================================
# 5) Interactive heatmap (dropdown on FAR RIGHT)
# =======================================================
branches = branch_summary['STORE_NAME'].tolist()  # same sort order as table
init_branch = branches[0] if branches else None

def build_heatmap(branch):
    dfb = activity_full[activity_full['STORE_NAME'] == branch]
    pivot = (
        dfb.pivot(index='Till_Code', columns='TIME_ONLY', values='Receipts')
           .reindex(columns=intervals)
           .fillna(0)
    )

    # non-zero labels only
    text_labels = np.where(pivot.values > 0, pivot.values.astype(int).astype(str), "")

    fig = px.imshow(
        pivot,
        labels=dict(x="Time of Day (30-min slot)", y="Till", color="Receipts"),
        x=[t.strftime('%H:%M') for t in pivot.columns],
        y=pivot.index,
        color_continuous_scale='Blues',
        aspect='auto',
        title=f"üïí Half-Hourly Till Activity ‚Äî {branch}",
        text_auto=False
    )

    fig.update_traces(
        text=text_labels,
        texttemplate="%{text}",
        textfont=dict(size=10),
        hovertemplate="Till: %{y}<br>Time: %{x}<br>Receipts: %{z}<extra></extra>"
    )

    # more right margin to host the far-right dropdown
    fig.update_xaxes(side='top', tickangle=45)
    fig.update_layout(
        height=max(450, 25*len(pivot.index)),
        margin=dict(l=150, r=200, t=80, b=40),
        coloraxis_colorbar=dict(title="Unique Receipts")
    )

    return fig

fig = build_heatmap(init_branch)

# Dropdown buttons
buttons = []
for b in branches:
    dfb = activity_full[activity_full['STORE_NAME'] == b]
    pivot = (
        dfb.pivot(index='Till_Code', columns='TIME_ONLY', values='Receipts')
           .reindex(columns=intervals)
           .fillna(0)
    )
    text_labels = np.where(pivot.values > 0, pivot.values.astype(int).astype(str), "")
    buttons.append(dict(
        label=b,
        method='update',
        args=[
            {
                'z':[pivot.values],
                'x':[[t.strftime('%H:%M') for t in pivot.columns]],
                'y':[pivot.index],
                'text':[text_labels]
            },
            {
                'title': f"üïí Half-Hourly Till Activity ‚Äî {b}"
            }
        ]
    ))

# Far-right dropdown (x > 1.0)
fig.update_layout(
    updatemenus=[dict(
        type='dropdown',
        x=1.20, xanchor='left',     # FAR RIGHT
        y=1.15, yanchor='top',
        buttons=buttons,
        showactive=True,
        direction='down',
        bgcolor='rgba(255,255,255,0.95)',
        bordercolor='lightgray',
        font=dict(size=12),
        pad=dict(r=4, t=4, b=4, l=4)
    )]
)

fig.show()

# =======================================================
# 6) Show numbered branch table
# =======================================================
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1500)
display(branch_summary_display[['#','STORE_NAME','Total_Transactions','Avg_Per_Till','Max_Per_Till','Busiest_Till','Total_Receipts']])

"""##Till Usage"""

import pandas as pd
import numpy as np
import plotly.express as px
from datetime import timedelta

# =======================================================
# 1Ô∏è‚É£ Prepare and validate data
# =======================================================
df['TRN_DATE'] = pd.to_datetime(df['TRN_DATE'], errors='coerce')
df = df.dropna(subset=['TRN_DATE']).copy()

for c in ['STORE_NAME', 'Till_Code', 'CUST_CODE']:
    if c not in df.columns:
        raise ValueError(f"Missing required column: {c}")
    df[c] = df[c].astype(str).str.strip()

# Create 30-minute interval buckets
df['TIME_SLOT'] = df['TRN_DATE'].dt.floor('30T')
df['TIME_ONLY'] = df['TIME_SLOT'].dt.time

# =======================================================
# 2Ô∏è‚É£ Aggregate: unique receipts per till every 30 mins
# =======================================================
till_activity = (
    df.groupby(['STORE_NAME', 'Till_Code', 'TIME_ONLY'], as_index=False)
      .agg(Receipts=('CUST_CODE', 'nunique'))
)

# =======================================================
# 3Ô∏è‚É£ Build full 30-minute grid (00:00 ‚Üí 23:30)
# =======================================================
start_time = pd.Timestamp("00:00:00")
intervals = [(start_time + timedelta(minutes=30*i)).time() for i in range(48)]

grid = (
    df[['STORE_NAME','Till_Code']]
    .drop_duplicates()
    .assign(key=1)
    .merge(pd.DataFrame({'TIME_ONLY': intervals, 'key':1}), on='key')
    .drop('key', axis=1)
)

activity_full = (
    grid.merge(till_activity, on=['STORE_NAME','Till_Code','TIME_ONLY'], how='left')
        .fillna({'Receipts':0})
)

# =======================================================
# 4Ô∏è‚É£ Summary metrics (with new columns)
# =======================================================
branch_summary = (
    till_activity.groupby('STORE_NAME', as_index=False)
        .agg(
            Store_Total_Receipts=('Receipts','sum'),
            Avg_Per_Till=('Receipts','mean'),
            Max_Per_Till=('Receipts','max'),
            Unique_Tills=('Till_Code','nunique')  # ‚úÖ New column
        )
)

# Find busiest till for each store
busiest = (
    till_activity.groupby(['STORE_NAME','Till_Code'], as_index=False)['Receipts'].sum()
    .sort_values(['STORE_NAME','Receipts'], ascending=[True,False])
    .groupby('STORE_NAME').first().reset_index()
    .rename(columns={'Receipts':'Busiest_Till_Receipts','Till_Code':'Busiest_Till'})
)

branch_summary = branch_summary.merge(busiest, on='STORE_NAME', how='left')

# % contribution of busiest till
branch_summary['Busiest_Till_Pct'] = np.where(
    branch_summary['Store_Total_Receipts'] > 0,
    100 * branch_summary['Busiest_Till_Receipts'] / branch_summary['Store_Total_Receipts'],
    0.0
)

# Sort & number branches
branch_summary = branch_summary.sort_values('Store_Total_Receipts', ascending=False).reset_index(drop=True)
branch_summary.index = np.arange(1, len(branch_summary) + 1)
branch_summary.index.name = '#'

# Format columns
branch_summary['Store_Total_Receipts'] = branch_summary['Store_Total_Receipts'].map('{:,.0f}'.format)
branch_summary['Busiest_Till_Receipts'] = branch_summary['Busiest_Till_Receipts'].map('{:,.0f}'.format)
branch_summary['Avg_Per_Till'] = branch_summary['Avg_Per_Till'].round(2)
branch_summary['Max_Per_Till'] = branch_summary['Max_Per_Till'].astype(int)
branch_summary['Busiest_Till_Pct'] = branch_summary['Busiest_Till_Pct'].round(1).map(lambda x: f"{x:.1f}%")

# =======================================================
# 5Ô∏è‚É£ Interactive heatmap with dropdown (far right)
# =======================================================
branches = branch_summary.reset_index()[['#', 'STORE_NAME']].astype({'#': str})
branches['Label'] = branches['#'] + '. ' + branches['STORE_NAME']
branch_list = branches['STORE_NAME'].tolist()
init_branch = branch_list[0]

def build_heatmap(branch):
    dfb = activity_full[activity_full['STORE_NAME'] == branch]
    pivot = (
        dfb.pivot(index='Till_Code', columns='TIME_ONLY', values='Receipts')
           .reindex(columns=intervals)
           .fillna(0)
    )

    fig = px.imshow(
        pivot,
        labels=dict(x="Time of Day (30-min slot)", y="Till", color="Receipts"),
        x=[t.strftime('%H:%M') for t in pivot.columns],
        y=pivot.index,
        color_continuous_scale='YlGnBu',
        aspect='auto',
        title=f"üïí Half-Hourly Till Activity ‚Äî {branch}"
    )

    fig.update_xaxes(side='top', tickangle=45)
    fig.update_layout(
        height=max(450, 25*len(pivot.index)),
        margin=dict(l=150, r=220, t=60, b=40),
        coloraxis_colorbar=dict(title="Unique Receipts"),
        font=dict(size=11),
        title_x=0.1,
    )
    return fig

fig = build_heatmap(init_branch)

# Dropdown menu (far right)
buttons = []
for i, b in enumerate(branch_list, start=1):
    dfb = activity_full[activity_full['STORE_NAME'] == b]
    pivot = (
        dfb.pivot(index='Till_Code', columns='TIME_ONLY', values='Receipts')
           .reindex(columns=intervals)
           .fillna(0)
    )
    label = f"{i}. {b}"
    buttons.append(dict(
        label=label,
        method='update',
        args=[{
            'z':[pivot.values],
            'x':[[t.strftime('%H:%M') for t in pivot.columns]],
            'y':[pivot.index]
        },{
            'title': f"üïí Half-Hourly Till Activity ‚Äî {b}"
        }]
    ))

fig.update_layout(
    updatemenus=[dict(
        type='dropdown',
        x=1.32, xanchor='left',
        y=1.12, yanchor='top',
        buttons=buttons,
        showactive=True,
        direction='down',
        bgcolor='rgba(255,255,255,0.95)',
        bordercolor='lightgray',
        font=dict(size=11)
    )]
)

fig.show()

# =======================================================
# 6Ô∏è‚É£ Display Final Summary Table (with Unique Tills)
# =======================================================
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1500)

display(branch_summary[['STORE_NAME',
                        'Unique_Tills',
                        'Store_Total_Receipts',
                        'Busiest_Till',
                        'Busiest_Till_Receipts',
                        'Busiest_Till_Pct',
                        'Avg_Per_Till',
                        'Max_Per_Till']].rename(
    columns={
        'Store_Total_Receipts': 'Store_Total_Receipts (unique)',
        'Busiest_Till_Receipts': 'Busiest_Till_Receipts (unique)'
    })
)

"""##Tax Compliance"""

import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go

# ===============================
# 1) Data prep & compliance flag
# ===============================
df['TRN_DATE'] = pd.to_datetime(df['TRN_DATE'], errors='coerce')
df = df.dropna(subset=['TRN_DATE']).copy()

for c in ['STORE_NAME','Till_Code','CUST_CODE','CU_DEVICE_SERIAL']:
    if c not in df.columns:
        raise ValueError(f"Missing required column: {c}")
    df[c] = df[c].astype(str).str.strip()

# Compliant if CU_DEVICE_SERIAL is present (non-empty after strip)
df['Tax_Compliant'] = np.where(
    df['CU_DEVICE_SERIAL'].replace({'nan':'', 'NaN':'', 'None':''}).str.len() > 0,
    'Compliant', 'Non-Compliant'
)

# ===============================
# 2) Global pie (optional)
# ===============================
global_summary = (
    df.groupby('Tax_Compliant', as_index=False)
      .agg(Receipts=('CUST_CODE','nunique'))
)

fig_global = px.pie(
    global_summary,
    names='Tax_Compliant',
    values='Receipts',
    color='Tax_Compliant',
    color_discrete_map={'Compliant':'#2ca02c','Non-Compliant':'#d62728'},
    hole=0.45,
    title='üåç Global Tax Compliance Overview'
)
fig_global.update_traces(textinfo='label+percent', pull=[0.05, 0])
fig_global.update_layout(showlegend=True)
fig_global.show()

# ===============================
# 3) Store & Till compliance data
# ===============================
store_till = (
    df.groupby(['STORE_NAME','Till_Code','Tax_Compliant'], as_index=False)
      .agg(Receipts=('CUST_CODE','nunique'))
)

# ===============================
# 4) Interactive till chart (show non-compliant counts)
# ===============================
branches = sorted(store_till['STORE_NAME'].unique().tolist())
init_branch = branches[0]

def build_compliance_plot(branch):
    dfb = store_till[store_till['STORE_NAME'] == branch]
    pivot = (
        dfb.pivot(index='Till_Code', columns='Tax_Compliant', values='Receipts')
           .fillna(0)
           .reindex(columns=['Compliant','Non-Compliant'], fill_value=0)
           .sort_values('Compliant', ascending=False)
    )

    fig = go.Figure()

    # ‚úÖ Compliant bars (green)
    fig.add_bar(
        y=pivot.index,
        x=pivot['Compliant'],
        name='Compliant',
        orientation='h',
        marker_color='#2ca02c',
        text=None,
        textposition='none'
    )

    # ‚úÖ Non-compliant bars (red) with visible counts (even if 0)
    fig.add_bar(
        y=pivot.index,
        x=pivot['Non-Compliant'],
        name='Non-Compliant',
        orientation='h',
        marker_color='#d62728',
        text=pivot['Non-Compliant'].astype(int).astype(str),  # show numbers including zeros
        textposition='outside',
        textfont=dict(size=10, color='black')
    )

    fig.update_layout(
        barmode='stack',
        title=f"üè™ Tax Compliance by Till ‚Äî {branch}",
        xaxis_title="Number of Unique Receipts",
        yaxis_title="Till Code",
        height=max(420, 24*len(pivot.index)),
        margin=dict(l=150, r=240, t=60, b=40),
        legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),
        font=dict(size=11),
    )

    fig.update_layout(annotations=[])  # clear any stray annotations
    return fig

fig_store = build_compliance_plot(init_branch)

# Dropdown at far right
buttons = []
for b in branches:
    dfb = store_till[store_till['STORE_NAME'] == b]
    pivot = (
        dfb.pivot(index='Till_Code', columns='Tax_Compliant', values='Receipts')
           .fillna(0)
           .reindex(columns=['Compliant','Non-Compliant'], fill_value=0)
           .sort_values('Compliant', ascending=False)
    )

    buttons.append(dict(
        label=b,
        method='update',
        args=[
            {
                'x':[pivot['Compliant'], pivot['Non-Compliant']],
                'y':[pivot.index, pivot.index],
                'text':[None, pivot['Non-Compliant'].astype(int).astype(str)],
            },
            {
                'title': f"üè™ Tax Compliance by Till ‚Äî {b}",
                'annotations': []
            }
        ]
    ))

fig_store.update_layout(
    updatemenus=[dict(
        type='dropdown',
        x=1.34, xanchor='left',   # üëâ far right placement
        y=1.15, yanchor='top',
        buttons=buttons,
        showactive=True,
        direction='down',
        bgcolor='rgba(255,255,255,0.95)',
        bordercolor='lightgray',
        font=dict(size=11)
    )]
)

fig_store.show()

# ===============================
# 5) Numbered, tidy store table
# ===============================
store_summary = (
    df.groupby(['STORE_NAME','Tax_Compliant'], as_index=False)
      .agg(Receipts=('CUST_CODE','nunique'))
    .pivot(index='STORE_NAME', columns='Tax_Compliant', values='Receipts')
    .fillna(0)
)
if 'Compliant' not in store_summary.columns:
    store_summary['Compliant'] = 0
if 'Non-Compliant' not in store_summary.columns:
    store_summary['Non-Compliant'] = 0

store_summary['Total'] = store_summary['Compliant'] + store_summary['Non-Compliant']
store_summary['Compliance_%'] = np.where(
    store_summary['Total'] > 0,
    (store_summary['Compliant'] / store_summary['Total'] * 100).round(1),
    0.0
)

store_summary = store_summary.sort_values(['Total','Compliance_%'], ascending=[False, False])
display_table = store_summary.reset_index()
display_table.insert(0, '#', range(1, len(display_table)+1))

# Format numbers neatly
display_table['Compliant'] = display_table['Compliant'].map('{:,.0f}'.format)
display_table['Non-Compliant'] = display_table['Non-Compliant'].map('{:,.0f}'.format)
display_table['Total'] = display_table['Total'].map('{:,.0f}'.format)
display_table['Compliance_%'] = display_table['Compliance_%'].map(lambda v: f"{v:.1f}%")

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1500)
display(display_table[['#','STORE_NAME','Compliant','Non-Compliant','Total','Compliance_%']])

"""#OTHER INSIGHTS

##Customer Baskets Overview
"""

import pandas as pd
import numpy as np
import plotly.express as px
import ipywidgets as widgets
from IPython.display import display, clear_output

# ===========================================
# 1Ô∏è‚É£ Data preparation
# ===========================================
df['QTY'] = pd.to_numeric(df['QTY'], errors='coerce').fillna(0)
df['NET_SALES'] = pd.to_numeric(df['NET_SALES'], errors='coerce').fillna(0)
df = df.dropna(subset=['ITEM_NAME','CUST_CODE','STORE_NAME','DEPARTMENT'])

# Exclude "LUGGAGE & BAGS"
df = df[df['DEPARTMENT'].str.upper() != 'LUGGAGE & BAGS']


# ===========================================
# 2Ô∏è‚É£ Helper functions
# ===========================================

def get_top_items(data, metric, top_x, departments=None):
    """
    Return top X items based on baskets + totals; sort by chosen metric.
    Adds a numbered '#' column safely on reruns.
    """
    temp = data.copy()
    if departments and 'ALL' not in departments:
        temp = temp[temp['DEPARTMENT'].isin(departments)]

    # Count unique baskets (CUST_CODE)
    basket_count = temp.groupby('ITEM_NAME')['CUST_CODE'].nunique().rename('Count_of_Baskets')
    agg_data = temp.groupby('ITEM_NAME')[['QTY','NET_SALES']].sum()

    merged = basket_count.to_frame().join(agg_data).reset_index()
    merged = merged.sort_values(metric, ascending=False).head(top_x)

    # Safe numbering on reruns
    if '#' in merged.columns:
        merged = merged.drop(columns=['#'])
    merged.insert(0, '#', range(1, len(merged) + 1))
    return merged

def compare_branch(global_top, branch_top, store_name):
    """
    Find global top items missing/underperforming in a store.
    Adds a numbered '#' column safely on reruns.
    """
    global_items = set(global_top['ITEM_NAME'])
    branch_items = set(branch_top['ITEM_NAME'])
    missing = global_items - branch_items

    missing_df = global_top[global_top['ITEM_NAME'].isin(missing)].copy()
    missing_df['STORE_NAME'] = store_name
    missing_df['Status'] = 'Missing/Underperforming'

    # Safe numbering on reruns
    if '#' in missing_df.columns:
        missing_df = missing_df.drop(columns=['#'])
    missing_df.insert(0, '#', range(1, len(missing_df) + 1))
    return missing_df

def fmt_table(df_in):
    """Add comma formatting to numeric columns (without changing data types for logic elsewhere)."""
    df = df_in.copy()
    for col in ['QTY', 'NET_SALES', 'Count_of_Baskets']:
        if col in df.columns:
            df[col] = df[col].map('{:,.0f}'.format)
    return df

# ===========================================
# 3Ô∏è‚É£ Widgets ‚Äî Compact layout
# ===========================================
branch_dropdown = widgets.Dropdown(
    options=sorted(df['STORE_NAME'].unique().tolist()),
    description='Branch:',
    layout=widgets.Layout(width='280px')
)
metric_dropdown = widgets.Dropdown(
    options=['QTY','NET_SALES'],
    value='QTY',
    description='Metric:',
    layout=widgets.Layout(width='200px')
)
departments = sorted(df['DEPARTMENT'].unique().tolist())
department_select = widgets.SelectMultiple(
    options=['ALL'] + departments,
    value=('ALL',),
    description='Dept:',
    layout=widgets.Layout(width='320px', height='120px')
)
topx_input = widgets.BoundedIntText(
    value=10, min=5, max=200, step=5,
    description='Top X:',
    layout=widgets.Layout(width='140px')
)
run_button = widgets.Button(
    description='üîç Analyze',
    button_style='success',
    layout=widgets.Layout(width='140px', height='36px')
)
controls_row = widgets.HBox(
    [branch_dropdown, metric_dropdown, topx_input, department_select, run_button],
    layout=widgets.Layout(margin='0', align_items='center')
)

# ===========================================
# 4Ô∏è‚É£ Core analysis + visual
# ===========================================
def run_analysis(_=None):
    clear_output(wait=True)
    display(controls_row)

    metric = metric_dropdown.value
    top_x = topx_input.value
    selected_branch = branch_dropdown.value
    selected_depts = list(department_select.value)

    # --- Global Top
    global_top = get_top_items(df, metric, top_x, selected_depts)
    global_top['STORE_NAME'] = 'GLOBAL'

    # --- Branch Top
    branch_df = df[df['STORE_NAME'] == selected_branch]
    branch_top = get_top_items(branch_df, metric, top_x, selected_depts)
    branch_top['STORE_NAME'] = selected_branch

    # --- Missing or underperforming
    missing_df = compare_branch(global_top, branch_top, selected_branch)

    # --- Combine for visualization
    combined = pd.concat([
        global_top.assign(Level='Global'),
        branch_top.assign(Level='Branch')
    ], ignore_index=True)

    # ===========================================
    # üü¶üü© Chart (commas + legend as color key)
    # ===========================================
    fig = px.bar(
        combined,
        x=metric, y='ITEM_NAME',
        color='Level',
        orientation='h',
        text='Count_of_Baskets',
        color_discrete_map={'Global':'#1f77b4','Branch':'#2ca02c'},
        title=f"üèÜ Top {top_x} Items by {metric} ‚Äî {selected_branch} vs Global"
    )
    fig.update_traces(texttemplate='%{text:,}', textposition='outside', cliponaxis=False)
    fig.update_layout(
        height=max(420, 22*len(branch_top)),
        xaxis_title=f"{metric} Value",
        yaxis_title="Item Name",
        barmode='group',
        legend_title_text="Color Key (Level)",
        legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='left', x=0),
        margin=dict(l=240, r=30, t=50, b=30)
    )
    fig.update_xaxes(tickformat=',')
    fig.show()

    # ===========================================
    # ‚ö†Ô∏è Missing Items Chart
    # ===========================================
    if len(missing_df) > 0:
        fig_missing = px.bar(
            missing_df,
            x='Count_of_Baskets',
            y='ITEM_NAME',
            orientation='h',
            text='Count_of_Baskets',
            title=f"‚ö†Ô∏è Items in Global Top {top_x} but Missing/Underperforming in {selected_branch}",
            color_discrete_sequence=['#d62728']
        )
        fig_missing.update_traces(texttemplate='%{text:,}', textposition='outside', cliponaxis=False)
        fig_missing.update_layout(
            height=max(420, 22*len(missing_df)),
            xaxis_title="Count of Baskets (Global)",
            yaxis_title="Item Name",
            margin=dict(l=240, r=30, t=50, b=30),
            legend_title_text="Color Key"
        )
        fig_missing.update_xaxes(tickformat=',')
        fig_missing.show()
    else:
        print(f"‚úÖ All top {top_x} global items are also among {selected_branch}'s top performers.")

    # ===========================================
    # üßæ Numbered Tables (1..N) with commas
    # ===========================================
    print("\nüîπ Global Top Items:")
    display(fmt_table(global_top[['#','ITEM_NAME','Count_of_Baskets','QTY','NET_SALES']])
            .style.background_gradient(cmap='Blues'))

    print(f"\nüîπ {selected_branch} Top Items:")
    display(fmt_table(branch_top[['#','ITEM_NAME','Count_of_Baskets','QTY','NET_SALES']])
            .style.background_gradient(cmap='Greens'))

    print(f"\n‚ö†Ô∏è Items Missing or Underperforming in {selected_branch}:")
    display(fmt_table(missing_df[['#','ITEM_NAME','Count_of_Baskets','QTY','NET_SALES']])
            .style.background_gradient(cmap='Reds'))

# ===========================================
# 7Ô∏è‚É£ Run + bind button
# ===========================================
run_button.on_click(run_analysis)
display(controls_row)
run_analysis()

"""##Global Category Overview-Sales"""

# =======================================================
# üè¨ Category Contribution Across All Branches (Numbered, % table + alpha-sorted chart)
# =======================================================
import pandas as pd
import numpy as np
import plotly.express as px
from IPython.display import display

# --- 1) Validate & prep ---
need = ['STORE_NAME', 'CATEGORY', 'NET_SALES']
missing = [c for c in need if c not in df.columns]
if missing:
    raise ValueError(f"Missing required columns: {missing}")

dc = df.copy()

# Clean numeric
dc['NET_SALES'] = (
    dc['NET_SALES'].astype(str)
       .str.replace(',', '', regex=False)
       .str.strip()
)
dc['NET_SALES'] = pd.to_numeric(dc['NET_SALES'], errors='coerce').fillna(0)

# Clean strings
for c in ['STORE_NAME','CATEGORY']:
    dc[c] = dc[c].astype(str).str.strip()

# Exclude CATEGORY that is nan/empty/"nan"
bad_mask = dc['CATEGORY'].isin(['', 'nan', 'NaN', 'None']) | dc['CATEGORY'].isna()
dc = dc[~bad_mask].copy()

# --- 2) Aggregate totals per branch-category ---
summary = (
    dc.groupby(['STORE_NAME','CATEGORY'], as_index=False)
      .agg(Total_Sales=('NET_SALES','sum'))
)

# Branch totals and % inside each branch
summary['Branch_Total'] = summary.groupby('STORE_NAME')['Total_Sales'].transform('sum')
summary['Pct_of_Branch'] = (summary['Total_Sales'] / summary['Branch_Total'] * 100)

# --- 3) Pivot to % table (rows=branch, cols=category), add numbering and TOTAL row ---
# Per-branch % pivot
pivot_pct = (
    summary.pivot(index='STORE_NAME', columns='CATEGORY', values='Pct_of_Branch')
           .fillna(0.0)
)

# Sort branches alphabetically
pivot_pct = pivot_pct.sort_index()

# Compute a GLOBAL "TOTAL" row as % of global sales per category
global_cat = dc.groupby('CATEGORY', as_index=False)['NET_SALES'].sum().rename(columns={'NET_SALES':'Global_Sales'})
global_total = float(global_cat['Global_Sales'].sum())
global_row = (global_cat
              .assign(Pct_Global=lambda d: np.where(global_total>0, 100*d['Global_Sales']/global_total, 0.0))
              .set_index('CATEGORY')['Pct_Global'])

# Align global row to pivot columns (missing cats -> 0)
global_row_aligned = pivot_pct.columns.to_series().map(global_row).fillna(0.0)
pivot_pct_with_total = pd.concat(
    [pivot_pct, pd.DataFrame([global_row_aligned.values], columns=pivot_pct.columns, index=['TOTAL'])],
    axis=0
)

# Insert numbering column for branches (skip numbering TOTAL)
numbering = [str(i+1) for i in range(len(pivot_pct))] + ['']
pivot_pct_with_total.insert(0, '#', numbering)

# Format as % strings
fmt_tbl = pivot_pct_with_total.copy()
for col in fmt_tbl.columns[1:]:
    fmt_tbl[col] = fmt_tbl[col].map(lambda x: f"{float(x):.2f}%")

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1500)
print("üìä Category Contribution (%) by Branch (numbered, with TOTAL row)")
display(fmt_tbl)

# --- 4) Visualization: stacked horizontal bar (branches alpha-sorted) ---
# Use alpha-sorted branch order
branches_sorted = sorted(summary['STORE_NAME'].unique())

fig = px.bar(
    summary.sort_values('STORE_NAME'),
    y='STORE_NAME',
    x='Total_Sales',
    color='CATEGORY',
    orientation='h',
    title='üè¨ Category Contribution by Branch',
    color_discrete_sequence=px.colors.qualitative.Safe
)

fig.update_layout(
    barmode='stack',
    height=max(500, 25*len(branches_sorted)),
    xaxis_title='Total Sales (KSh)',
    yaxis_title='Branch',
    legend_title='Category',
    margin=dict(l=200, r=40, t=60, b=40),
)
fig.update_xaxes(tickprefix='KSh ', tickformat=',.0f')
# Ensure alphabetical branch order on Y axis
fig.update_yaxes(categoryorder='array', categoryarray=branches_sorted)

fig.show()

"""##Global Category Overview-Baskets"""

# =======================================================
# üß∫ Category Contribution Across Branches ‚Äî Based on Baskets (Unique Receipts)
# =======================================================
import pandas as pd
import numpy as np
import plotly.express as px
from IPython.display import display

# --- 1Ô∏è‚É£ Validate & prep ---
need = ['STORE_NAME', 'CATEGORY', 'CUST_CODE']
missing = [c for c in need if c not in df.columns]
if missing:
    raise ValueError(f"Missing required columns: {missing}")

db = df.copy()

for c in ['STORE_NAME','CATEGORY','CUST_CODE']:
    db[c] = db[c].astype(str).str.strip()

# Exclude empty or nan categories
bad_mask = db['CATEGORY'].isin(['', 'nan', 'NaN', 'None']) | db['CATEGORY'].isna()
db = db[~bad_mask].copy()

# --- 2Ô∏è‚É£ Compute per-branch, per-category unique basket counts ---
summary_baskets = (
    db.groupby(['STORE_NAME','CATEGORY'], as_index=False)
      .agg(Baskets=('CUST_CODE','nunique'))
)

# Branch total baskets
summary_baskets['Branch_Total_Baskets'] = summary_baskets.groupby('STORE_NAME')['Baskets'].transform('sum')
summary_baskets['Pct_of_Branch'] = (
    (summary_baskets['Baskets'] / summary_baskets['Branch_Total_Baskets']) * 100
)

# --- 3Ô∏è‚É£ Pivot into % table ---
pivot_pct = (
    summary_baskets.pivot(index='STORE_NAME', columns='CATEGORY', values='Pct_of_Branch')
           .fillna(0.0)
)
pivot_pct = pivot_pct.sort_index()  # alphabetical

# Compute GLOBAL TOTAL row ‚Äî % of all baskets contributed by each category
global_cat = db.groupby('CATEGORY', as_index=False)['CUST_CODE'].nunique().rename(columns={'CUST_CODE':'Global_Baskets'})
global_total = float(global_cat['Global_Baskets'].sum())
global_row = (global_cat
              .assign(Pct_Global=lambda d: np.where(global_total>0, 100*d['Global_Baskets']/global_total, 0.0))
              .set_index('CATEGORY')['Pct_Global'])
global_row_aligned = pivot_pct.columns.to_series().map(global_row).fillna(0.0)

# Add numbering and TOTAL
pivot_pct_with_total = pd.concat(
    [pivot_pct, pd.DataFrame([global_row_aligned.values], columns=pivot_pct.columns, index=['TOTAL'])],
    axis=0
)
numbering = [str(i+1) for i in range(len(pivot_pct))] + ['']
pivot_pct_with_total.insert(0, '#', numbering)

# --- 4Ô∏è‚É£ Format for display ---
fmt_tbl = pivot_pct_with_total.copy()
for col in fmt_tbl.columns[1:]:
    fmt_tbl[col] = fmt_tbl[col].map(lambda x: f"{float(x):.2f}%")

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1500)
print("üìä Category Contribution by Baskets (%) per Branch (numbered, with TOTAL row)")
display(fmt_tbl)

# --- 5Ô∏è‚É£ Visualization: stacked horizontal bar (branches alphabetical) ---
branches_sorted = sorted(summary_baskets['STORE_NAME'].unique())

fig = px.bar(
    summary_baskets.sort_values('STORE_NAME'),
    y='STORE_NAME',
    x='Baskets',
    color='CATEGORY',
    orientation='h',
    title='üß∫ Category Contribution by Branch (Based on Baskets)',
    color_discrete_sequence=px.colors.qualitative.Safe
)

fig.update_layout(
    barmode='stack',
    height=max(500, 25*len(branches_sorted)),
    xaxis_title='Number of Unique Baskets',
    yaxis_title='Branch',
    legend_title='Category',
    margin=dict(l=200, r=40, t=60, b=40),
)
fig.update_yaxes(categoryorder='array', categoryarray=branches_sorted)
fig.show()

"""##Supplier Contribution"""

# =======================================================
# üß∫ Supplier basket share with scoped filters
# - Category/Department lists only where supplier is active
# - Total_Baskets_All = baskets in current filters (ignores supplier)
# - % = baskets with supplier / baskets in current filters
# =======================================================
import pandas as pd
import numpy as np
import plotly.express as px
import ipywidgets as widgets
from IPython.display import display, clear_output

# --- 1) Validate & prep ---
need = ['STORE_NAME', 'CUST_CODE', 'SUPPLIER_NAME', 'CATEGORY', 'DEPARTMENT']
missing = [c for c in need if c not in df.columns]
if missing:
    raise ValueError(f"Missing required columns in df: {missing}")

ds = df.copy()
for c in need:
    ds[c] = ds[c].astype(str).str.strip()

# remove blank supplier rows
bad_sup = ds['SUPPLIER_NAME'].isin(['', 'nan', 'NaN', 'None']) | ds['SUPPLIER_NAME'].isna()
ds = ds[~bad_sup].copy()

# --- 2) Widgets ---
supplier_options = sorted(ds['SUPPLIER_NAME'].unique().tolist())
sup_dd  = widgets.Dropdown(options=supplier_options, description='Supplier:', layout=widgets.Layout(width='360px'))
cat_dd  = widgets.Dropdown(options=['ALL'], description='Category:', layout=widgets.Layout(width='260px'))
dept_dd = widgets.Dropdown(options=['ALL'], description='Department:', layout=widgets.Layout(width='260px'))

controls = widgets.HBox([sup_dd, cat_dd, dept_dd], layout=widgets.Layout(margin='0 0 8px 0'))
out = widgets.Output()
display(controls, out)

def _set_options(widget, options, keep='ALL'):
    """Set options and keep current value if still valid; else default."""
    options = list(dict.fromkeys(options))  # de-dup preserving order
    if keep is not None and keep in options:
        widget.options = options
        widget.value = keep
    else:
        widget.options = options
        widget.value = options[0] if options else None

def _refresh_category_options():
    """Categories only where supplier is active (global)."""
    sup = sup_dd.value
    cats = ['ALL'] + sorted(
        ds.loc[ds['SUPPLIER_NAME'] == sup, 'CATEGORY'].dropna().unique().tolist()
    )
    _set_options(cat_dd, cats, keep=cat_dd.value)

def _refresh_department_options():
    """Departments only where supplier is active; if category picked, within that category."""
    sup = sup_dd.value
    filt = ds[ds['SUPPLIER_NAME'] == sup]
    if cat_dd.value != 'ALL':
        filt = filt[filt['CATEGORY'] == cat_dd.value]
    depts = ['ALL'] + sorted(filt['DEPARTMENT'].dropna().unique().tolist())
    _set_options(dept_dd, depts, keep=dept_dd.value)

def _render(_=None):
    with out:
        clear_output(wait=True)

        supplier = sup_dd.value
        cat_sel  = cat_dd.value
        dep_sel  = dept_dd.value

        # ---- Build the *filter scope* (ignores supplier in denominator) ----
        scope = ds.copy()
        if cat_sel != 'ALL':
            scope = scope[scope['CATEGORY'] == cat_sel]
        if dep_sel != 'ALL':
            scope = scope[scope['DEPARTMENT'] == dep_sel]

        # Denominator: baskets in scope per branch (unique CUST_CODE)
        total_baskets_all = (
            scope.groupby('STORE_NAME')['CUST_CODE'].nunique()
                 .rename('Total_Baskets_All')
        )

        # Numerator: baskets in scope that include the supplier
        baskets_with_supplier = (
            scope.loc[scope['SUPPLIER_NAME'] == supplier, ['STORE_NAME','CUST_CODE']]
                 .drop_duplicates()
                 .groupby('STORE_NAME')['CUST_CODE'].nunique()
                 .rename('Baskets_With_Supplier')
        )

        # Merge and compute %
        tbl = (total_baskets_all.to_frame()
               .merge(baskets_with_supplier, left_index=True, right_index=True, how='left')
               .fillna({'Baskets_With_Supplier': 0})
               .reset_index().rename(columns={'index':'STORE_NAME'}))

        tbl['Supplier_Share_%'] = np.where(
            tbl['Total_Baskets_All'] > 0,
            (tbl['Baskets_With_Supplier'] / tbl['Total_Baskets_All'] * 100).round(2),
            0.0
        )

        # Sort by % desc
        tbl = tbl.sort_values('Supplier_Share_%', ascending=False).reset_index(drop=True)

        # TOTAL row (over all branches, still using same scope)
        g_total = int(total_baskets_all.sum())
        g_with  = int(baskets_with_supplier.sum())
        g_pct   = round(100 * g_with / g_total, 2) if g_total else 0.0

        total_row = pd.DataFrame([{
            'STORE_NAME': 'TOTAL',
            'Total_Baskets_All': g_total,
            'Baskets_With_Supplier': g_with,
            'Supplier_Share_%': g_pct
        }])

        final = pd.concat([total_row, tbl], ignore_index=True)

        # Number branches (skip TOTAL)
        numbering = [''] + [str(i) for i in range(1, len(final))]
        final.insert(0, '#', numbering)

        # Pretty format
        disp = final.copy()
        for c in ['Total_Baskets_All','Baskets_With_Supplier']:
            disp[c] = disp[c].apply(lambda x: f"{int(x):,}" if pd.notna(x) else x)
        disp['Supplier_Share_%'] = disp['Supplier_Share_%'].apply(lambda x: f"{float(x):.2f}%")

        scope_text = []
        scope_text.append(f"Supplier = {supplier}")
        scope_text.append(f"Category = {cat_sel}" if cat_sel!='ALL' else "Category = ALL")
        scope_text.append(f"Department = {dep_sel}" if dep_sel!='ALL' else "Department = ALL")

        print("üìä Supplier share of baskets within filter scope")
        print("   " + " | ".join(scope_text))
        display(disp[['#','STORE_NAME','Total_Baskets_All','Baskets_With_Supplier','Supplier_Share_%']])

        # Chart (alphabetical branches)
        vis = tbl.sort_values('STORE_NAME').copy()
        fig = px.bar(
            vis, x='Supplier_Share_%', y='STORE_NAME',
            orientation='h',
            text=vis['Supplier_Share_%'].map(lambda v: f"{v:.2f}%"),
            color_discrete_sequence=['#2ca02c'],
            title=f"{supplier} ‚Äî Share of Baskets (within filter scope)"
        )
        fig.update_traces(textposition='outside', cliponaxis=False)
        fig.update_layout(
            xaxis_title='% of Baskets',
            yaxis_title='Branch',
            height=max(460, 22*vis['STORE_NAME'].nunique()),
            margin=dict(l=220, r=40, t=60, b=40),
            showlegend=False
        )
        fig.show()

# --- Bindings ---
def _on_supplier_change(_):
    _refresh_category_options()
    _refresh_department_options()
    _render()

def _on_category_change(_):
    _refresh_department_options()
    _render()

def _on_department_change(_):
    _render()

sup_dd.observe(lambda ch: _on_supplier_change(ch) if ch['name']=='value' else None, names='value')
cat_dd.observe(lambda ch: _on_category_change(ch) if ch['name']=='value' else None, names='value')
dept_dd.observe(lambda ch: _on_department_change(ch) if ch['name']=='value' else None, names='value')

# initial populate + render
_refresh_category_options()
_refresh_department_options()
_render()

"""##Category Overview"""

# =======================================================
# üß∫ Supplier contribution (% of baskets) by Category ‚Üí Department ‚Üí Branch
# (‚úÖ Single filter row ‚Äî no duplication)
# =======================================================
import pandas as pd
import numpy as np
import plotly.express as px
import ipywidgets as widgets
from IPython.display import display, clear_output

# -------- 1) Validate & prep --------
need = ['STORE_NAME', 'CUST_CODE', 'SUPPLIER_NAME', 'CATEGORY', 'DEPARTMENT']
missing = [c for c in need if c not in df.columns]
if missing:
    raise ValueError(f"Missing required columns in df: {missing}")

ds = df.copy()
for c in need:
    ds[c] = ds[c].astype(str).str.strip()

# drop empty suppliers
bad_sup = ds['SUPPLIER_NAME'].isin(['', 'nan', 'NaN', 'None']) | ds['SUPPLIER_NAME'].isna()
ds = ds[~bad_sup].copy()

# -------- 2) Widgets --------
cat_options  = ['ALL'] + sorted(ds['CATEGORY'].dropna().unique().tolist())
cat_dd  = widgets.Dropdown(options=cat_options, description='Category:', layout=widgets.Layout(width='300px'))
dept_dd = widgets.Dropdown(options=['ALL'], description='Department:', layout=widgets.Layout(width='280px'))
bran_dd = widgets.Dropdown(options=['ALL'], description='Branch:',    layout=widgets.Layout(width='280px'))

controls = widgets.HBox([cat_dd, dept_dd, bran_dd], layout=widgets.Layout(margin='0 0 8px 0'))
out = widgets.Output()
display(controls, out)

def _set_options(widget, options, keep='ALL'):
    options = list(dict.fromkeys(options))
    if keep in options:
        widget.options = options
        widget.value = keep
    else:
        widget.options = options
        widget.value = options[0] if options else None

def _refresh_dept_options():
    d = ds if cat_dd.value == 'ALL' else ds[ds['CATEGORY'] == cat_dd.value]
    depts = ['ALL'] + sorted(d['DEPARTMENT'].dropna().unique().tolist())
    _set_options(dept_dd, depts, keep=dept_dd.value)

def _refresh_branch_options():
    d = ds.copy()
    if cat_dd.value != 'ALL':
        d = d[d['CATEGORY'] == cat_dd.value]
    if dept_dd.value != 'ALL':
        d = d[d['DEPARTMENT'] == dept_dd.value]
    branches = ['ALL'] + sorted(d['STORE_NAME'].dropna().unique().tolist())
    _set_options(bran_dd, branches, keep=bran_dd.value)

def _render(_=None):
    with out:
        clear_output(wait=True)

        # ----- Build scope -----
        scope = ds.copy()
        if cat_dd.value != 'ALL':
            scope = scope[scope['CATEGORY'] == cat_dd.value]
        if dept_dd.value != 'ALL':
            scope = scope[scope['DEPARTMENT'] == dept_dd.value]
        if bran_dd.value != 'ALL':
            scope = scope[scope['STORE_NAME'] == bran_dd.value]

        if scope.empty:
            print("No baskets match the selected filters.")
            return

        denom_baskets = scope[['CUST_CODE']].drop_duplicates()
        total_baskets_scope = denom_baskets['CUST_CODE'].nunique()

        sup_baskets = (
            scope[['SUPPLIER_NAME','CUST_CODE']]
            .drop_duplicates()
            .groupby('SUPPLIER_NAME', as_index=False)['CUST_CODE']
            .nunique()
            .rename(columns={'CUST_CODE':'Baskets_With_Supplier'})
        )

        sup_baskets['Supplier_Share_%'] = np.where(
            total_baskets_scope > 0,
            (sup_baskets['Baskets_With_Supplier'] / total_baskets_scope * 100).round(2),
            0.0
        )

        sup_baskets = sup_baskets.sort_values('Supplier_Share_%', ascending=False).reset_index(drop=True)
        sup_baskets.insert(0, '#', range(1, len(sup_baskets)+1))

        tbl = sup_baskets.copy()
        tbl['Baskets_With_Supplier'] = tbl['Baskets_With_Supplier'].map('{:,}'.format)
        tbl['Supplier_Share_%'] = tbl['Supplier_Share_%'].map(lambda v: f"{v:.2f}%")

        scope_bits = []
        scope_bits.append(f"Category: {cat_dd.value}")
        scope_bits.append(f"Department: {dept_dd.value}")
        scope_bits.append(f"Branch: {bran_dd.value}")
        print("üìä Supplier Contribution ‚Äî % of Baskets within scope")
        print("   " + " | ".join(scope_bits))
        print(f"   Denominator (unique baskets in scope): {total_baskets_scope:,}\n")

        display(tbl[['#','SUPPLIER_NAME','Baskets_With_Supplier','Supplier_Share_%']])

        chart = sup_baskets.copy()
        order = chart['SUPPLIER_NAME'].tolist()

        fig = px.bar(
            chart,
            x='Supplier_Share_%',
            y='SUPPLIER_NAME',
            orientation='h',
            text=chart['Supplier_Share_%'].map(lambda v: f"{v:.2f}%"),
            color_discrete_sequence=['#1f77b4'],
            title='Supplier Share of Baskets'
        )
        fig.update_traces(textposition='outside', cliponaxis=False)
        fig.update_layout(
            yaxis=dict(categoryorder='array', categoryarray=order),
            xaxis_title='% of Baskets',
            yaxis_title='Supplier',
            height=max(500, 22*len(order)),
            margin=dict(l=260, r=40, t=60, b=40),
            showlegend=False
        )
        fig.show()

# ---- Bindings ----
def _on_cat(ch):
    if ch['name'] == 'value':
        _refresh_dept_options()
        _refresh_branch_options()
        _render()

def _on_dept(ch):
    if ch['name'] == 'value':
        _refresh_branch_options()
        _render()

def _on_branch(ch):
    if ch['name'] == 'value':
        _render()

cat_dd.observe(_on_cat, names='value')
dept_dd.observe(_on_dept, names='value')
bran_dd.observe(_on_branch, names='value')

# ---- Initialize ----
_refresh_dept_options()
_refresh_branch_options()
_render()

"""##Branch Comparison"""

import pandas as pd
import numpy as np
import plotly.express as px
import ipywidgets as widgets
from IPython.display import display, clear_output

# ==================================================
# 1) Data prep (expects df already in memory)
#    Required cols: ITEM_NAME, CUST_CODE, STORE_NAME, DEPARTMENT, QTY, NET_SALES
# ==================================================
df['QTY'] = pd.to_numeric(df['QTY'], errors='coerce').fillna(0)
df['NET_SALES'] = pd.to_numeric(df['NET_SALES'], errors='coerce').fillna(0)
df = df.dropna(subset=['ITEM_NAME','CUST_CODE','STORE_NAME','DEPARTMENT'])

# Exclude LUGGAGE & BAGS
df = df[df['DEPARTMENT'].str.upper() != 'LUGGAGE & BAGS']

# ==================================================
# 2) Helpers
# ==================================================
def aggregate_by_item(data, departments=None):
    """
    Returns per-ITEM_NAME totals: Count_of_Baskets, QTY, NET_SALES
    Respects the departments filter (unless 'ALL' in selection).
    """
    temp = data.copy()
    if departments and 'ALL' not in departments:
        temp = temp[temp['DEPARTMENT'].isin(departments)]

    baskets = temp.groupby('ITEM_NAME')['CUST_CODE'].nunique().rename('Count_of_Baskets')
    totals  = temp.groupby('ITEM_NAME')[['QTY','NET_SALES']].sum()

    out = baskets.to_frame().join(totals, how='outer').fillna(0).reset_index()
    return out

def get_top_items(data, metric, top_x, departments=None):
    """
    Build Top X table by chosen metric (QTY or NET_SALES) using item-level aggregates.
    Also includes Count_of_Baskets. Returns numbered (1..N) dataframe.
    """
    agg = aggregate_by_item(data, departments)
    if metric not in ['QTY','NET_SALES']:
        metric = 'QTY'
    agg = agg.sort_values(metric, ascending=False).head(top_x).copy()

    if '#' in agg.columns:
        agg = agg.drop(columns=['#'])
    agg.insert(0, '#', range(1, len(agg)+1))
    return agg

def compare_zero_only(top_source, agg_target, name_source, name_target):
    """
    From SOURCE Top X, keep only items that are COMPLETELY ZERO in TARGET
    (zero baskets, zero QTY, zero NET_SALES). Returns numbered df.
    """
    other = agg_target[['ITEM_NAME','Count_of_Baskets','QTY','NET_SALES']].copy()
    other.columns = ['ITEM_NAME','Target_Count_of_Baskets','Target_QTY','Target_NET_SALES']

    merged = top_source.merge(other, on='ITEM_NAME', how='left')
    for col in ['Target_Count_of_Baskets','Target_QTY','Target_NET_SALES']:
        merged[col] = merged[col].fillna(0)

    mask_zero = (
        (merged['Target_Count_of_Baskets'] == 0) &
        (merged['Target_QTY'] == 0) &
        (merged['Target_NET_SALES'] == 0)
    )
    out = merged.loc[mask_zero].copy()
    # Rename source columns for clarity
    out = out.rename(columns={
        'Count_of_Baskets': 'Source_Count_of_Baskets',
        'QTY': 'Source_QTY',
        'NET_SALES': 'Source_NET_SALES',
    })
    out['Source_Top'] = name_source
    out['Zero_In'] = name_target

    if '#' in out.columns: out = out.drop(columns=['#'])
    out.insert(0, '#', range(1, len(out)+1))
    return out

def fmt_table(df_in):
    """Comma-format numeric columns for display."""
    df = df_in.copy()
    for col in ['Source_Count_of_Baskets','Source_QTY','Source_NET_SALES',
                'Target_Count_of_Baskets','Target_QTY','Target_NET_SALES']:
        if col in df.columns:
            df[col] = df[col].map('{:,.0f}'.format)
    return df

def build_zero_table(zero_df, source_label, target_label, metric):
    """
    Presentable table for ‚ÄúTop in <source> but ZERO in <target>‚Äù
    Sorted by the selected metric from the source side.
    """
    cols = ['#','ITEM_NAME','Source_Count_of_Baskets','Source_QTY','Source_NET_SALES',
            'Target_Count_of_Baskets','Target_QTY','Target_NET_SALES','Source_Top','Zero_In']

    if zero_df.empty:
        return pd.DataFrame(columns=cols)

    tbl = zero_df[cols].copy()
    sort_col = 'Source_QTY' if metric == 'QTY' else 'Source_NET_SALES'
    tbl = tbl.sort_values(sort_col, ascending=False).reset_index(drop=True)
    tbl['#'] = range(1, len(tbl)+1)
    tbl['Source_Top'] = source_label
    tbl['Zero_In'] = target_label
    return tbl

# ==================================================
# 3) Controls (compact)
# ==================================================
branches = sorted(df['STORE_NAME'].unique().tolist())
departments = sorted(df['DEPARTMENT'].unique().tolist())

branch_A = widgets.Dropdown(options=branches, description='Branch A:', layout=widgets.Layout(width='280px'))
branch_B = widgets.Dropdown(options=branches, description='Branch B:', layout=widgets.Layout(width='280px'))
metric_dd = widgets.Dropdown(options=['QTY','NET_SALES'], value='QTY', description='Metric:', layout=widgets.Layout(width='200px'))
topx_box = widgets.BoundedIntText(value=10, min=5, max=200, step=5, description='Top X:', layout=widgets.Layout(width='140px'))
dept_sel = widgets.SelectMultiple(options=['ALL'] + departments, value=('ALL',), description='Dept:', layout=widgets.Layout(width='320px', height='120px'))
run_btn = widgets.Button(description='üîç Compare', button_style='success', layout=widgets.Layout(width='140px', height='36px'))

controls = widgets.HBox([branch_A, branch_B, metric_dd, topx_box, dept_sel, run_btn], layout=widgets.Layout(margin='0', align_items='center'))

# ==================================================
# 4) Runner
# ==================================================
def run(_=None):
    clear_output(wait=True)
    display(controls)

    a = branch_A.value
    b = branch_B.value
    metric = metric_dd.value
    top_x = topx_box.value
    chosen_depts = list(dept_sel.value)

    dfA = df[df['STORE_NAME'] == a]
    dfB = df[df['STORE_NAME'] == b]

    # Aggregates (respect dept filter)
    aggA = aggregate_by_item(dfA, chosen_depts)
    aggB = aggregate_by_item(dfB, chosen_depts)
    aggGLOBAL = aggregate_by_item(df, chosen_depts)  # global across all branches

    topA = get_top_items(dfA, metric, top_x, chosen_depts)
    topB = get_top_items(dfB, metric, top_x, chosen_depts)
    topGLOBAL = get_top_items(df, metric, top_x, chosen_depts)

    # ===== Chart: A vs B grouped horizontal (Top-X each) =====
    combA = topA.copy(); combA['Branch'] = a
    combB = topB.copy(); combB['Branch'] = b
    combined = pd.concat([combA, combB], ignore_index=True)

    fig = px.bar(
        combined,
        x=metric, y='ITEM_NAME',
        color='Branch',
        orientation='h',
        text='Source_Count_of_Baskets' if 'Source_Count_of_Baskets' in combined.columns else 'Count_of_Baskets',
        color_discrete_map={a:'#1f77b4', b:'#2ca02c'},
        title=f"üè¨ Branch Comparison ‚Äî Top {top_x} by {metric}: {a} vs {b}"
    )
    fig.update_traces(texttemplate='%{text:,}', textposition='outside', cliponaxis=False)
    fig.update_layout(
        height=max(460, 20*max(len(topA), len(topB))),
        xaxis_title=f"{metric} Value",
        yaxis_title="Item Name",
        barmode='group',
        legend_title_text="Color Key (Branch)",
        legend=dict(orientation='h', y=1.03, x=0),
        margin=dict(l=260, r=30, t=60, b=30)
    )
    fig.update_xaxes(tickformat=',')
    fig.show()

    # ===== Zero-only gaps between A and B =====
    zero_A_top_but_zero_in_B = compare_zero_only(topA, aggB, a, b)
    zero_B_top_but_zero_in_A = compare_zero_only(topB, aggA, b, a)

    tbl_A_zero_in_B = build_zero_table(zero_A_top_but_zero_in_B, a, b, metric)
    tbl_B_zero_in_A = build_zero_table(zero_B_top_but_zero_in_A, b, a, metric)

    # ===== NEW: Global Top-X but ZERO in each store =====
    zero_GLOBAL_but_zero_in_A = compare_zero_only(topGLOBAL, aggA, 'GLOBAL', a)
    zero_GLOBAL_but_zero_in_B = compare_zero_only(topGLOBAL, aggB, 'GLOBAL', b)

    tbl_GLOBAL_zero_in_A = build_zero_table(zero_GLOBAL_but_zero_in_A, 'GLOBAL', a, metric)
    tbl_GLOBAL_zero_in_B = build_zero_table(zero_GLOBAL_but_zero_in_B, 'GLOBAL', b, metric)

    # ----- Optional mini charts for zero-only sets -----
    def plot_zero(df_zero, title):
        if df_zero.empty:
            print(f"‚úÖ No gaps: {title} ‚Äî none.")
            return
        figz = px.bar(
            df_zero,
            x='Source_Count_of_Baskets',
            y='ITEM_NAME',
            orientation='h',
            text='Source_Count_of_Baskets',
            title=title,
            color_discrete_sequence=['#d62728']
        )
        figz.update_traces(texttemplate='%{text:,}', textposition='outside', cliponaxis=False)
        figz.update_layout(height=max(380, 18*len(df_zero)),
                           xaxis_title="Count of Baskets (Source)",
                           yaxis_title="Item Name",
                           margin=dict(l=260, r=30, t=60, b=30))
        figz.update_xaxes(tickformat=',')
        figz.show()

    plot_zero(tbl_A_zero_in_B, f"‚ö†Ô∏è Top in {a} but ZERO in {b} (baskets/qty/sales)")
    plot_zero(tbl_B_zero_in_A, f"‚ö†Ô∏è Top in {b} but ZERO in {a} (baskets/qty/sales)")
    plot_zero(tbl_GLOBAL_zero_in_A, f"üåç Top Globally but ZERO in {a} (baskets/qty/sales)")
    plot_zero(tbl_GLOBAL_zero_in_B, f"üåç Top Globally but ZERO in {b} (baskets/qty/sales)")

    # ===== Top-X tables for each branch =====
    print(f"\nüîπ {a} ‚Äî Top {top_x} by {metric}:")
    display(topA[['#','ITEM_NAME','Count_of_Baskets','QTY','NET_SALES']])

    print(f"\nüîπ {b} ‚Äî Top {top_x} by {metric}:")
    display(topB[['#','ITEM_NAME','Count_of_Baskets','QTY','NET_SALES']])

    print(f"\nüîπ üåç GLOBAL ‚Äî Top {top_x} by {metric}:")
    display(topGLOBAL[['#','ITEM_NAME','Count_of_Baskets','QTY','NET_SALES']])

    # ===== ZERO-in-other tables (ALWAYS DISPLAY) =====
    print(f"\nüß≠ Top in {a} but ZERO in {b}:")
    display(fmt_table(tbl_A_zero_in_B))

    print(f"\nüß≠ Top in {b} but ZERO in {a}:")
    display(fmt_table(tbl_B_zero_in_A))

    print(f"\nüß≠ üåç Top Globally but ZERO in {a}:")
    display(fmt_table(tbl_GLOBAL_zero_in_A))

    print(f"\nüß≠ üåç Top Globally but ZERO in {b}:")
    display(fmt_table(tbl_GLOBAL_zero_in_B))

# Bind + show
run_btn.on_click(run)
display(controls)
run()

"""##Product Perfomance"""

import pandas as pd
import numpy as np
import plotly.express as px
import ipywidgets as widgets
from IPython.display import display, clear_output

# =========================================
# 0) Prepare Data
# =========================================
required = ['ITEM_CODE','ITEM_NAME','CUST_CODE','STORE_NAME','QTY','CATEGORY','DEPARTMENT']
missing = [c for c in required if c not in df.columns]
if missing:
    raise ValueError(f"df is missing required columns: {missing}")

for c in ['ITEM_CODE','ITEM_NAME','CUST_CODE','STORE_NAME','CATEGORY','DEPARTMENT']:
    df[c] = df[c].astype(str).str.strip()
df['QTY'] = pd.to_numeric(df['QTY'], errors='coerce').fillna(0)

# =========================================
# 1) Widgets
# =========================================
item_lookup = (
    df[['ITEM_CODE','ITEM_NAME']].drop_duplicates()
      .sort_values(['ITEM_CODE','ITEM_NAME'])
)
item_options = [(f"{row.ITEM_CODE} ‚Äî {row.ITEM_NAME}", row.ITEM_CODE) for _, row in item_lookup.iterrows()]

item_dd = widgets.Combobox(
    options=[opt[0] for opt in item_options],
    placeholder='Type or pick an ITEM_CODE ‚Äî ITEM_NAME',
    description='SKU:',
    ensure_option=False,
    layout=widgets.Layout(width='520px')
)

show_chart_chk = widgets.Checkbox(value=True, description='Show stacked bar chart', indent=False)

run_btn = widgets.Button(
    description='üîé Analyze', button_style='success',
    layout=widgets.Layout(width='120px', height='36px', margin='0 0 0 10px')
)

controls = widgets.HBox([item_dd, show_chart_chk, run_btn], layout=widgets.Layout(margin='0'))

# =========================================
# 2) Main Analysis
# =========================================
def analyze_selected_item(_=None):
    clear_output(wait=True)
    display(controls)

    raw = (item_dd.value or '').strip()
    if not raw:
        print("Please type or select an ITEM_CODE.")
        return

    target_code = raw.split('‚Äî', 1)[0].strip() if '‚Äî' in raw else raw
    if target_code not in set(df['ITEM_CODE']):
        print(f"‚ùå ITEM_CODE '{target_code}' not found.")
        return

    item_data = df[df['ITEM_CODE'] == target_code]
    any_name = item_data['ITEM_NAME'].iloc[0]
    item_cat = item_data['CATEGORY'].mode().iloc[0]
    item_dept = item_data['DEPARTMENT'].mode().iloc[0]

    # ----------------------------
    # Basket / Store Computations
    # ----------------------------
    store_total_customers = df.groupby('STORE_NAME')['CUST_CODE'].nunique()
    cat_pool = df[df['CATEGORY'] == item_cat].groupby('STORE_NAME')['CUST_CODE'].nunique()
    dept_pool = df[df['DEPARTMENT'] == item_dept].groupby('STORE_NAME')['CUST_CODE'].nunique()

    basket_item_counts = (
        df.groupby(['STORE_NAME','CUST_CODE'])['ITEM_CODE']
          .nunique().rename('Distinct_SKUs').reset_index()
    )

    baskets_with_item = (
        item_data[['STORE_NAME','CUST_CODE']].drop_duplicates().assign(Has_Item=1)
    )

    with_comp = baskets_with_item.merge(basket_item_counts, on=['STORE_NAME','CUST_CODE'], how='left')
    with_comp['Only_Item'] = (with_comp['Distinct_SKUs'] == 1).astype(int)
    with_comp['With_Others'] = (with_comp['Distinct_SKUs'] > 1).astype(int)

    # Highest quantity in a basket per store
    max_qty_per_basket = (
        item_data.groupby(['STORE_NAME','CUST_CODE'])['QTY'].sum().groupby('STORE_NAME').max()
    )
    # Total quantity per store
    total_qty_per_store = item_data.groupby('STORE_NAME')['QTY'].sum()

    store_summary_counts = (
        with_comp.groupby('STORE_NAME', as_index=False)
                 .agg(Baskets_With_Item=('Has_Item','sum'),
                      Only_Item_Baskets=('Only_Item','sum'),
                      With_Other_Items=('With_Others','sum'))
    )

    store_summary_counts = (
        store_summary_counts
        .merge(max_qty_per_basket.rename('Highest_QTY_In_Basket'),
               on='STORE_NAME', how='left')
        .merge(total_qty_per_store.rename('Total_QTY_Sold_Branch'),
               on='STORE_NAME', how='left')
        .fillna({'Highest_QTY_In_Basket':0, 'Total_QTY_Sold_Branch':0})
    )

    # ----------------------------
    # Global Percentages
    # ----------------------------
    global_baskets = store_summary_counts['Baskets_With_Item'].sum()
    global_only = store_summary_counts['Only_Item_Baskets'].sum()
    global_with = store_summary_counts['With_Other_Items'].sum()

    global_only_pct = round(100 * global_only / global_baskets, 1) if global_baskets else 0.0
    global_with_pct = round(100 * global_with / global_baskets, 1) if global_baskets else 0.0

    global_store_customers = int(store_total_customers.sum())
    global_cat_customers = int(cat_pool.sum())
    global_dept_customers = int(dept_pool.sum())

    global_pct_store = round(100 * global_baskets / global_store_customers, 1) if global_store_customers else 0.0
    global_pct_cat = round(100 * global_baskets / global_cat_customers, 1) if global_cat_customers else 0.0
    global_pct_dept = round(100 * global_baskets / global_dept_customers, 1) if global_dept_customers else 0.0

    # ----------------------------
    # Per-store Percentages
    # ----------------------------
    per_store = store_summary_counts.copy()
    per_store['Only_Item_Baskets'] = (
        (per_store['Only_Item_Baskets'] / per_store['Baskets_With_Item'] * 100)
        .replace([np.inf, np.nan], 0).round(1)
    )
    per_store['With_Other_Items'] = (
        (per_store['With_Other_Items'] / per_store['Baskets_With_Item'] * 100)
        .replace([np.inf, np.nan], 0).round(1)
    )

    per_store['Pct_of_Store_Customers'] = np.where(
        per_store['STORE_NAME'].isin(store_total_customers.index),
        (per_store['Baskets_With_Item'] /
         per_store['STORE_NAME'].map(store_total_customers).replace(0, np.nan) * 100).round(1),
        0.0
    )
    per_store['Pct_of_Category_Customers'] = np.where(
        per_store['STORE_NAME'].isin(cat_pool.index),
        (per_store['Baskets_With_Item'] /
         per_store['STORE_NAME'].map(cat_pool).replace(0, np.nan) * 100).round(1),
        0.0
    )
    per_store['Pct_of_Department_Customers'] = np.where(
        per_store['STORE_NAME'].isin(dept_pool.index),
        (per_store['Baskets_With_Item'] /
         per_store['STORE_NAME'].map(dept_pool).replace(0, np.nan) * 100).round(1),
        0.0
    )

    per_store = per_store.sort_values('Baskets_With_Item', ascending=False).reset_index(drop=True)

    # ----------------------------
    # Totals Row
    # ----------------------------
    totals = pd.DataFrame([{
        'STORE_NAME': 'TOTAL',
        'Baskets_With_Item': store_summary_counts['Baskets_With_Item'].sum(),
        'Only_Item_Baskets': global_only_pct,
        'With_Other_Items': global_with_pct,
        'Highest_QTY_In_Basket': int(per_store['Highest_QTY_In_Basket'].max()),
        'Total_QTY_Sold_Branch': store_summary_counts['Total_QTY_Sold_Branch'].sum(),
        'Pct_of_Store_Customers': global_pct_store,
        'Pct_of_Category_Customers': global_pct_cat,
        'Pct_of_Department_Customers': global_pct_dept
    }])

    final = pd.concat([totals, per_store], ignore_index=True)
    final.insert(0, '#', ['' if i==0 else i for i in range(len(final))])

    # ----------------------------
    # Formatting
    # ----------------------------
    final['Baskets_With_Item'] = final['Baskets_With_Item'].map('{:,.0f}'.format)
    for col in ['Only_Item_Baskets','With_Other_Items',
                'Pct_of_Store_Customers','Pct_of_Category_Customers','Pct_of_Department_Customers']:
        final[col] = final[col].map(lambda x: f"{float(x):.1f}%")
    for col in ['Highest_QTY_In_Basket','Total_QTY_Sold_Branch']:
        final[col] = final[col].map('{:,.0f}'.format)

    # ----------------------------
    # Display Results
    # ----------------------------
    print(f"üß∫ Basket Composition for SKU {target_code} ‚Äî {any_name}")
    print(f"‚Ä¢ CATEGORY: {item_cat}   ‚Ä¢ DEPARTMENT: {item_dept}\n")

    display(final[['#','STORE_NAME','Baskets_With_Item',
                   'Only_Item_Baskets','With_Other_Items',
                   'Highest_QTY_In_Basket','Total_QTY_Sold_Branch',
                   'Pct_of_Store_Customers','Pct_of_Category_Customers','Pct_of_Department_Customers']])

    # ----------------------------
    # Optional Stacked Chart
    # ----------------------------
    if show_chart_chk.value:
        chart_df = per_store.melt(
            id_vars='STORE_NAME',
            value_vars=['Only_Item_Baskets','With_Other_Items'],
            var_name='Type', value_name='Percent'
        )
        chart_df['Type'] = chart_df['Type'].map({
            'Only_Item_Baskets': 'Only Item (%)',
            'With_Other_Items': 'With Other Items (%)'
        })

        fig = px.bar(
            chart_df,
            x='Percent', y='STORE_NAME',
            color='Type', orientation='h',
            color_discrete_map={'Only Item (%)':'#1f77b4','With Other Items (%)':'#ff7f0e'},
            title=f"Stores ‚Äî Basket Split (%) for {target_code} ({any_name})"
        )
        fig.update_layout(
            barmode='stack',
            height=max(420, 22*len(chart_df['STORE_NAME'].unique())),
            xaxis_title='Percentage of Baskets with SKU',
            yaxis_title='Store',
            legend_title_text='Basket Type',
            margin=dict(l=180, r=40, t=60, b=30)
        )
        fig.update_xaxes(tickformat=',')
        fig.show()

# =========================================
# 3) Run
# =========================================
run_btn.on_click(analyze_selected_item)
display(controls)
analyze_selected_item()

"""##Global Loyalty Overview"""

import pandas as pd
import numpy as np
import ipywidgets as widgets
from IPython.display import display, clear_output
from ipywidgets import Output

# =========================
# 0Ô∏è‚É£ Data Prep & Validation
# =========================
required = ['TRN_DATE','STORE_NAME','CUST_CODE','LOYALTY_CUSTOMER_CODE','NET_SALES']
missing = [c for c in required if c not in df.columns]
if missing:
    raise ValueError(f"df is missing required columns: {missing}")

dfL = df.copy()
dfL['TRN_DATE'] = pd.to_datetime(dfL['TRN_DATE'], errors='coerce')
dfL = dfL.dropna(subset=['TRN_DATE','STORE_NAME','CUST_CODE'])

for c in ['STORE_NAME','CUST_CODE','LOYALTY_CUSTOMER_CODE']:
    dfL[c] = dfL[c].astype(str).str.strip()

dfL['NET_SALES'] = pd.to_numeric(dfL['NET_SALES'], errors='coerce').fillna(0)

# Keep only rows with valid loyalty codes
dfL = dfL[dfL['LOYALTY_CUSTOMER_CODE'].replace({'nan':'', 'NaN':'', 'None':''}).str.len() > 0].copy()

# One record per (store, receipt, loyalty customer)
receipts = (
    dfL.groupby(['STORE_NAME','CUST_CODE','LOYALTY_CUSTOMER_CODE'], as_index=False)
       .agg(
           Basket_Value=('NET_SALES','sum'),
           First_Time=('TRN_DATE','min')
       )
)

# Helper: format integers
def fmt_int(df_in, cols):
    df = df_in.copy()
    for c in cols:
        if c in df:
            df[c] = df[c].map('{:,.0f}'.format)
    return df

"""##Branch Loyalty Overview"""

# =========================================
# 1Ô∏è‚É£ GLOBAL OVERVIEW TABLE
# =========================================
# Count customers with >1 baskets per branch
per_branch_multi = (
    receipts.groupby(['STORE_NAME','LOYALTY_CUSTOMER_CODE'])
            .agg(
                Baskets_in_Store=('CUST_CODE','nunique'),
                Total_Value_in_Store=('Basket_Value','sum')
            )
            .reset_index()
)

per_branch_multi = per_branch_multi[per_branch_multi['Baskets_in_Store'] > 1]

overview = (
    per_branch_multi.groupby('STORE_NAME', as_index=False)
      .agg(
          Loyal_Customers_Multi=('LOYALTY_CUSTOMER_CODE','nunique'),
          Total_Baskets_of_Those=('Baskets_in_Store','sum'),
          Total_Value_of_Those=('Total_Value_in_Store','sum')
      )
)

overview['Avg_Baskets_per_Customer'] = np.where(
    overview['Loyal_Customers_Multi'] > 0,
    overview['Total_Baskets_of_Those'] / overview['Loyal_Customers_Multi'],
    0.0
).round(2)

overview = overview.sort_values(
    ['Loyal_Customers_Multi','Total_Baskets_of_Those'],
    ascending=[False, False]
).reset_index(drop=True)

overview.insert(0, '#', range(1, len(overview)+1))

# Format table
ov_disp = fmt_int(overview, ['Loyal_Customers_Multi','Total_Baskets_of_Those','Total_Value_of_Those'])

print("üåç All-Branch Overview ‚Äî Loyalty Customers with >1 Baskets (table only)")
display(ov_disp[['#','STORE_NAME','Loyal_Customers_Multi','Total_Baskets_of_Those',
                 'Avg_Baskets_per_Customer','Total_Value_of_Those']])

"""##Customer Loyalty Overview"""

# ---------- Branch dropdown & single-table output (self-contained) ----------
import numpy as np
import pandas as pd
import ipywidgets as widgets
from ipywidgets import Output
from IPython.display import display

# --- Expect dfL and receipts from earlier steps; rebuild receipts if missing ---
if 'receipts' not in globals():
    required_cols = ['TRN_DATE','STORE_NAME','CUST_CODE','LOYALTY_CUSTOMER_CODE','NET_SALES']
    missing = [c for c in required_cols if c not in dfL.columns]
    if missing:
        raise ValueError(f"dfL is missing required columns: {missing}")

    dfTmp = dfL.copy()
    dfTmp['TRN_DATE'] = pd.to_datetime(dfTmp['TRN_DATE'], errors='coerce')
    dfTmp = dfTmp.dropna(subset=['TRN_DATE','STORE_NAME','CUST_CODE'])
    for c in ['STORE_NAME','CUST_CODE','LOYALTY_CUSTOMER_CODE']:
        dfTmp[c] = dfTmp[c].astype(str).str.strip()
    dfTmp['NET_SALES'] = pd.to_numeric(dfTmp['NET_SALES'], errors='coerce').fillna(0)

    # keep loyalty only
    dfTmp = dfTmp[dfTmp['LOYALTY_CUSTOMER_CODE'].replace({'nan':'', 'NaN':'', 'None':''}).str.len() > 0]

    receipts = (
        dfTmp.groupby(['STORE_NAME','CUST_CODE','LOYALTY_CUSTOMER_CODE'], as_index=False)
             .agg(Basket_Value=('NET_SALES','sum'),
                  First_Time=('TRN_DATE','min'))
    )

# --- Build global "stores per customer" lookup for multi-store flag ---
stores_per_cust = (
    receipts.groupby('LOYALTY_CUSTOMER_CODE')['STORE_NAME']
            .nunique()
            .reset_index(name='Stores_Visited')
)

# --- small int formatter ---
def fmt_int(df_in, cols):
    df = df_in.copy()
    for c in cols:
        if c in df:
            df[c] = df[c].map('{:,.0f}'.format)
    return df

# --- UI: branch dropdown + output ---
branch_out = Output()
branch_dd = widgets.Dropdown(
    options=sorted(receipts['STORE_NAME'].unique().tolist()),
    description='Branch:',
    layout=widgets.Layout(width='360px')
)

def render_branch_table(_=None):
    with branch_out:
        branch_out.clear_output()
        branch = branch_dd.value

        per_store = (
            receipts[receipts['STORE_NAME'] == branch]
            .groupby('LOYALTY_CUSTOMER_CODE', as_index=False)
            .agg(
                Baskets_in_Store=('CUST_CODE','nunique'),
                Total_Value_in_Store=('Basket_Value','sum'),
                First_Time=('First_Time','min'),
                Last_Time=('First_Time','max')
            )
        )

        # only customers with >1 baskets in this branch
        per_store = per_store[per_store['Baskets_in_Store'] > 1].copy()

        # add multi-store info
        per_store = per_store.merge(stores_per_cust, on='LOYALTY_CUSTOMER_CODE', how='left')
        per_store['Stores_Visited'] = per_store['Stores_Visited'].fillna(1).astype(int)
        per_store['In_Multiple_Stores'] = np.where(per_store['Stores_Visited'] > 1, 'Yes', 'No')

        # order + numbering
        per_store = per_store.sort_values(
            ['Baskets_in_Store','Total_Value_in_Store'], ascending=[False, False]
        ).reset_index(drop=True)
        per_store.insert(0, '#', range(1, len(per_store)+1))

        # format & display
        if per_store.empty:
            print(f"üü¢ No loyalty customers with more than one basket in {branch}.")
            return

        disp = per_store.copy()
        disp['First_Time'] = disp['First_Time'].dt.strftime('%Y-%m-%d %H:%M:%S')
        disp['Last_Time']  = disp['Last_Time'].dt.strftime('%Y-%m-%d %H:%M:%S')
        disp = fmt_int(disp, ['Baskets_in_Store','Total_Value_in_Store','Stores_Visited'])

        print(f"üè™ Branch Overview ‚Äî {branch} (Loyal customers with >1 baskets)  |  "
              f"Multi-store shoppers flagged")
        display(disp[['#','LOYALTY_CUSTOMER_CODE','Baskets_in_Store','Total_Value_in_Store',
                      'Stores_Visited','In_Multiple_Stores','First_Time','Last_Time']])

display(branch_dd)
display(branch_out)
render_branch_table()
branch_dd.observe(lambda ch: render_branch_table() if ch['name']=='value' else None, names='value')

# ---------- Global Loyalty Customer Drilldown (All Stores) ----------
cust_out = Output()

# Customers who have >1 baskets anywhere (global list)
global_multi_custs = (
    receipts.groupby('LOYALTY_CUSTOMER_CODE')['CUST_CODE']
            .nunique()
)
eligible_customers = sorted(global_multi_custs[global_multi_custs > 1].index.tolist())

cust_dd = widgets.Combobox(
    options=eligible_customers,
    placeholder='Type or select a loyalty customer code',
    description='Customer:',
    ensure_option=False,
    layout=widgets.Layout(width='360px')
)

def render_customer_detail(_=None):
    with cust_out:
        cust_out.clear_output()
        sel_cust = (cust_dd.value or '').strip()

        if not sel_cust:
            print("üîç Select a loyalty customer (with >1 baskets globally) to view receipts across stores.")
            return

        rc = receipts[receipts['LOYALTY_CUSTOMER_CODE'] == sel_cust].copy()
        if rc.empty:
            print(f"‚ùå No receipts found for loyalty customer '{sel_cust}'.")
            return

        # ---- Per-store summary for this customer ----
        per_store = (
            rc.groupby('STORE_NAME', as_index=False)
              .agg(
                  Baskets=('CUST_CODE','nunique'),
                  Total_Value=('Basket_Value','sum'),
                  First_Time=('First_Time','min'),
                  Last_Time=('First_Time','max')
              )
              .sort_values(['Baskets','Total_Value'], ascending=[False, False])
              .reset_index(drop=True)
        )
        per_store.insert(0, '#', range(1, len(per_store)+1))
        per_store_disp = per_store.copy()
        per_store_disp['First_Time'] = per_store_disp['First_Time'].dt.strftime('%Y-%m-%d %H:%M:%S')
        per_store_disp['Last_Time']  = per_store_disp['Last_Time'].dt.strftime('%Y-%m-%d %H:%M:%S')
        per_store_disp = fmt_int(per_store_disp, ['Baskets','Total_Value'])

        # ---- Receipt-level detail (all stores) ----
        rc = rc.sort_values(['STORE_NAME','First_Time']).reset_index(drop=True)
        rc_disp = rc.copy()
        rc_disp['Basket_Value'] = rc_disp['Basket_Value'].map('{:,.0f}'.format)
        rc_disp['First_Time'] = rc_disp['First_Time'].dt.strftime('%Y-%m-%d %H:%M:%S')

        print(f"üë§ Loyalty Customer ‚Äî {sel_cust}")
        print("üìå Per-Store Summary (All Stores)")
        display(per_store_disp[['#','STORE_NAME','Baskets','Total_Value','First_Time','Last_Time']])

        print("üßæ Receipt-Level Detail (All Stores)")
        display(rc_disp[['STORE_NAME','CUST_CODE','Basket_Value','First_Time']])

display(cust_dd)
display(cust_out)
render_customer_detail()
cust_dd.observe(lambda ch: render_customer_detail() if ch['name']=='value' else None, names='value')

"""##Global Pricing Overview"""

# =======================================================
# üßæ High-Level Store Summary ‚Äî Multi-Priced SKUs per Day
# (ONLY branches where Price_Spread > 0; includes TOTAL row)
# =======================================================
import pandas as pd
import numpy as np
import plotly.express as px
from IPython.display import display

# 0Ô∏è‚É£ --- Prep ---
dfp = df.copy()
dfp['TRN_DATE'] = pd.to_datetime(dfp['TRN_DATE'], errors='coerce')
dfp = dfp.dropna(subset=['TRN_DATE','STORE_NAME','ITEM_CODE','ITEM_NAME','QTY','SP_PRE_VAT'])

for c in ['STORE_NAME','ITEM_CODE','ITEM_NAME']:
    dfp[c] = dfp[c].astype(str).str.strip()

# Normalize numeric fields
dfp['SP_PRE_VAT'] = (
    dfp['SP_PRE_VAT'].astype(str)
        .str.replace(',', '', regex=False)
        .str.strip()
)
dfp['SP_PRE_VAT'] = pd.to_numeric(dfp['SP_PRE_VAT'], errors='coerce').fillna(0.0)
dfp['QTY']        = pd.to_numeric(dfp['QTY'],        errors='coerce').fillna(0.0)

dfp['DATE'] = dfp['TRN_DATE'].dt.date

# 1Ô∏è‚É£ --- Compute per (store, date, item) ---
grp = (
    dfp.groupby(['STORE_NAME','DATE','ITEM_CODE','ITEM_NAME'], as_index=False)
       .agg(
           Num_Prices=('SP_PRE_VAT', lambda s: s.dropna().nunique()),
           Price_Min=('SP_PRE_VAT', 'min'),
           Price_Max=('SP_PRE_VAT', 'max'),
           Total_QTY=('QTY', 'sum')
       )
)

# Keep only true multi-price rows with positive spread
grp['Price_Spread'] = (grp['Price_Max'] - grp['Price_Min']).round(2)
multi_price = grp[(grp['Num_Prices'] > 1) & (grp['Price_Spread'] > 0)].copy()

# Compute value impact for those rows
multi_price['Diff_Value'] = (multi_price['Total_QTY'] * multi_price['Price_Spread']).round(2)

# 2Ô∏è‚É£ --- Aggregate high-level summary per store (ONLY where spread > 0)
summary = (
    multi_price.groupby('STORE_NAME', as_index=False)
               .agg(
                   Items_with_MultiPrice=('ITEM_CODE','nunique'),
                   Total_Diff_Value=('Diff_Value','sum'),
                   Avg_Spread=('Price_Spread','mean'),
                   Max_Spread=('Price_Spread','max')
               )
)

# Sort & number
summary = summary.sort_values('Total_Diff_Value', ascending=False).reset_index(drop=True)
summary.insert(0, '#', range(1, len(summary)+1))

# 3Ô∏è‚É£ --- Add TOTAL row (sum + max rules)
total_row = pd.DataFrame({
    '#': [''],
    'STORE_NAME': ['TOTAL'],
    'Items_with_MultiPrice': [int(summary['Items_with_MultiPrice'].sum())],
    'Total_Diff_Value': [float(summary['Total_Diff_Value'].sum())],
    'Avg_Spread': [float(summary['Avg_Spread'].max() if not summary.empty else 0.0)],
    'Max_Spread': [float(summary['Max_Spread'].max() if not summary.empty else 0.0)]
})
summary_total = pd.concat([summary, total_row], ignore_index=True)

# 4Ô∏è‚É£ --- Format for display
fmt = summary_total.copy()
def _fmt_int(x):
    try: return f"{int(x):,}"
    except: return x
def _fmt_2(x):
    try: return f"{float(x):,.2f}"
    except: return x

fmt['Items_with_MultiPrice'] = fmt['Items_with_MultiPrice'].apply(_fmt_int)
for c in ['Total_Diff_Value','Avg_Spread','Max_Spread']:
    fmt[c] = fmt[c].apply(_fmt_2)

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1500)

print("üè¨ High-Level Store Summary ‚Äî Items Sold at Multiple Prices per Day (Spread > 0 only)")
display(fmt[['#','STORE_NAME','Items_with_MultiPrice','Total_Diff_Value','Avg_Spread','Max_Spread']])

# 5Ô∏è‚É£ --- Visualization: Top Stores by Value Difference (spread > 0)
topN = min(20, len(summary))
if topN > 0:
    fig = px.bar(
        summary.head(topN).sort_values('Total_Diff_Value', ascending=True),
        x='Total_Diff_Value',
        y='STORE_NAME',
        orientation='h',
        text='Total_Diff_Value',
        color='Items_with_MultiPrice',
        color_continuous_scale='Blues',
        title='Top Stores by Value Impact from Multi-Priced SKUs (Spread > 0)'
    )
    fig.update_traces(texttemplate='KSh %{text:,.2f}', textposition='outside', cliponaxis=False)
    fig.update_layout(
        xaxis_title='Total Value Difference (KSh)',
        yaxis_title='Store Name',
        coloraxis_colorbar=dict(title='SKUs with >1 Price'),
        height=max(450, 20*topN),
        margin=dict(l=200, r=30, t=60, b=40)
    )
    fig.update_xaxes(tickprefix='KSh ', tickformat=',.2f')
    fig.show()
else:
    print("‚úÖ No multi-priced items with positive spread found across stores.")

"""##Branch Brach Overview"""

# =======================================================
# üîé Branch Drilldown (clean view): All SKUs with >1 price in a day
# Counts distinct prices using currency rounding (2dp, HALF_UP)
# Only shows rows where Price_Spread > 0
# =======================================================
import pandas as pd
import numpy as np
from decimal import Decimal, ROUND_HALF_UP
import ipywidgets as widgets
from IPython.display import display, clear_output

# ---------- Helpers ----------
def _round2(v: float) -> float:
    """Stable currency rounding to 2dp (HALF_UP) to avoid float noise."""
    return float(Decimal(str(v)).quantize(Decimal('0.01'), rounding=ROUND_HALF_UP))

def fmt_commas(df_in, cols, decimals=0):
    df = df_in.copy()
    for c in cols:
        if c in df:
            if decimals == 0:
                df[c] = df[c].map(lambda v: f"{int(v):,}" if pd.notna(v) else v)
            else:
                df[c] = df[c].map(lambda v: f"{float(v):,.{decimals}f}" if pd.notna(v) else v)
    return df

# --- Prep base frame (idempotent if re-run) ---
d0 = df.copy()
d0['TRN_DATE'] = pd.to_datetime(d0['TRN_DATE'], errors='coerce')

# Keep only clean rows
d0 = d0.dropna(subset=['TRN_DATE','STORE_NAME','ITEM_CODE','ITEM_NAME','QTY','SP_PRE_VAT','CUST_CODE'])

for c in ['STORE_NAME','ITEM_CODE','ITEM_NAME','CUST_CODE']:
    d0[c] = d0[c].astype(str).str.strip()

# Normalize numeric fields
d0['SP_PRE_VAT'] = (
    d0['SP_PRE_VAT'].astype(str)
      .str.replace(',', '', regex=False)
      .str.strip()
)
d0['SP_PRE_VAT'] = pd.to_numeric(d0['SP_PRE_VAT'], errors='coerce').fillna(0.0)
d0['QTY'] = pd.to_numeric(d0['QTY'], errors='coerce').fillna(0.0)
d0['DATE'] = d0['TRN_DATE'].dt.date

# Currency-rounded price used for ALL counting/aggregations
d0['PRICE2'] = d0['SP_PRE_VAT'].map(_round2)

# ---------- UI ----------
branch_dd = widgets.Dropdown(
    options=sorted(d0['STORE_NAME'].unique().tolist()),
    description='Branch:',
    layout=widgets.Layout(width='360px')
)

# Optional: ignore non-positive quantities (returns/voids)
ignore_nonpositive_qty = widgets.Checkbox(
    value=False, description='Ignore non-positive QTY', indent=False
)

run_btn = widgets.Button(
    description='Show',
    button_style='primary',
    layout=widgets.Layout(width='120px', height='34px')
)

controls = widgets.HBox(
    [
        widgets.VBox([branch_dd, ignore_nonpositive_qty]),
        widgets.HBox([run_btn], layout=widgets.Layout(width='200px', justify_content='flex-end', align_items='flex-end'))
    ],
    layout=widgets.Layout(margin='0 0 6px 0', align_items='flex-end', justify_content='space-between')
)

out = widgets.Output()
display(controls, out)

def render(_=None):
    with out:
        out.clear_output()
        store = branch_dd.value
        if not store:
            print("Select a branch.")
            return

        d = d0[d0['STORE_NAME'] == store].copy()
        if ignore_nonpositive_qty.value:
            d = d[d['QTY'] > 0].copy()

        if d.empty:
            print("No rows for this branch.")
            return

        # ================= A) Per (DATE, ITEM) summary =================
        # Use PRICE2 for distinct counts and min/max to avoid float micro-differences.
        per_item_day = (
            d.groupby(['DATE','ITEM_CODE','ITEM_NAME'], as_index=False)
             .agg(
                 Num_Prices=('PRICE2', lambda s: s.dropna().nunique()),  # distinct rounded prices
                 Price_Min=('PRICE2', 'min'),
                 Price_Max=('PRICE2', 'max'),
                 Total_QTY=('QTY', 'sum')
             )
        )
        per_item_day['Price_Spread'] = per_item_day['Price_Max'] - per_item_day['Price_Min']

        # Keep only true multi-price with positive spread
        multi = per_item_day[(per_item_day['Num_Prices'] > 1) & (per_item_day['Price_Spread'] > 0)].copy()
        if multi.empty:
            print(f"‚úÖ {store}: No SKUs with more than one distinct price (spread > 0) on the same day.")
            return

        # Round for presentation
        multi['Price_Min'] = multi['Price_Min'].map(_round2)
        multi['Price_Max'] = multi['Price_Max'].map(_round2)
        multi['Price_Spread'] = multi['Price_Spread'].map(_round2)
        multi['Diff_Value']   = (multi['Total_QTY'] * multi['Price_Spread']).map(_round2)

        # ------- Summary table (ordered, clean) -------
        multi_sum = multi.sort_values(['DATE','Price_Spread','Total_QTY'], ascending=[False, False, False]).reset_index(drop=True)
        multi_sum.insert(0, '#', range(1, len(multi_sum)+1))
        multi_disp = multi_sum.copy()
        multi_disp = fmt_commas(multi_disp, ['Total_QTY'], decimals=0)
        multi_disp = fmt_commas(multi_disp, ['Price_Min','Price_Max','Price_Spread','Diff_Value'], decimals=2)

        sku_days  = len(multi_disp)
        sku_count = multi[['ITEM_CODE','ITEM_NAME']].drop_duplicates().shape[0]
        value_sum = float(multi['Diff_Value'].sum())

        print(f"üè™ Branch: {store}")
        print(f"‚Ä¢ Item-Days with >1 price (spread>0): {sku_days:,}   ‚Ä¢ Distinct SKUs affected: {sku_count:,}   ‚Ä¢ Total Diff Value: {value_sum:,.2f}\n")

        print("üßæ Per Item/Day ‚Äî Summary (min/max price, spread, total qty, diff value)")
        display(multi_disp[['#','DATE','ITEM_CODE','ITEM_NAME','Num_Prices','Price_Min','Price_Max','Price_Spread','Total_QTY','Diff_Value']])

        # ================= B) Detailed price breakdown =================
        # Build price-level breakdown only for the (DATE, ITEM) that passed the spread>0 rule
        ok_keys = multi[['DATE','ITEM_CODE']]
        price_brk = (
            d.merge(ok_keys, on=['DATE','ITEM_CODE'], how='inner')
             .groupby(['DATE','ITEM_CODE','ITEM_NAME','PRICE2'], as_index=False)
             .agg(
                 Qty_At_Price=('QTY','sum'),
                 Receipts_At_Price=('CUST_CODE','nunique'),
                 First_Time=('TRN_DATE','min'),
                 Last_Time=('TRN_DATE','max')
             )
        )

        price_brk = price_brk.sort_values(['DATE','ITEM_NAME','PRICE2'], ascending=[False, True, False]).reset_index(drop=True)
        price_view = price_brk.copy()
        price_view['First_Time'] = price_view['First_Time'].dt.strftime('%Y-%m-%d %H:%M:%S')
        price_view['Last_Time']  = price_view['Last_Time'].dt.strftime('%Y-%m-%d %H:%M:%S')

        # Format + rename for display
        price_view['Price'] = price_view['PRICE2'].map(lambda v: f"{v:,.2f}")
        price_view = price_view.drop(columns=['PRICE2'])
        price_view['Qty_At_Price'] = price_view['Qty_At_Price'].map(lambda v: f"{int(v):,}")
        price_view['Receipts_At_Price'] = price_view['Receipts_At_Price'].map(lambda v: f"{int(v):,}")
        price_view['Time_Window'] = price_view['First_Time'] + ' ‚Üí ' + price_view['Last_Time']

        # Final compact columns (clean & readable)
        price_compact = price_view[['DATE','ITEM_CODE','ITEM_NAME','Price','Qty_At_Price','Receipts_At_Price','Time_Window']] \
            .rename(columns={'Qty_At_Price':'QTY','Receipts_At_Price':'Receipts'})

        print("üîç Detailed ‚Äî Price Breakdown (clean view)")
        display(price_compact)

run_btn.on_click(render)

# Auto-render for default selection if available
if branch_dd.value:
    render()

"""##Global Refunds Overview"""

import pandas as pd
import numpy as np
import ipywidgets as widgets
from IPython.display import display, clear_output

# ======================================================
# 0Ô∏è‚É£ Data Prep
# ======================================================
req_cols = ['STORE_NAME','CAP_CUSTOMER_CODE','NET_SALES']
missing = [c for c in req_cols if c not in df.columns]
if missing:
    raise ValueError(f"Missing required columns: {missing}")

d = df.copy()
d['NET_SALES'] = (
    d['NET_SALES'].astype(str)
      .str.replace(',', '', regex=False)
      .str.strip()
)
d['NET_SALES'] = pd.to_numeric(d['NET_SALES'], errors='coerce').fillna(0)

for c in ['STORE_NAME','CAP_CUSTOMER_CODE']:
    d[c] = d[c].astype(str).str.strip()

# Determine Sale Type
d['Sale_Type'] = np.where(
    d['CAP_CUSTOMER_CODE'].replace({'nan':'','NaN':'','None':''})=='',
    'General sales', 'On_account sales'
)

# Focus only on negative receipts
neg = d[d['NET_SALES'] < 0].copy()

# ======================================================
# 1Ô∏è‚É£ Group and Aggregate
# ======================================================
has_count_col = 'CUST_COUNT' in neg.columns

# Aggregation per Store + Sale Type + Customer Code
group_cols = ['STORE_NAME','Sale_Type','CAP_CUSTOMER_CODE']
val_summ = neg.groupby(group_cols)['NET_SALES'].sum().rename('Total_Neg_Value')

if has_count_col:
    cnt_summ = neg.groupby(group_cols)['CUST_COUNT'].sum().rename('Total_Count')
elif 'CUST_CODE' in neg.columns:
    cnt_summ = neg.groupby(group_cols)['CUST_CODE'].nunique().rename('Total_Count')
else:
    cnt_summ = neg.groupby(group_cols).size().rename('Total_Count')

summary = pd.concat([val_summ, cnt_summ], axis=1).reset_index()
summary['Abs_Neg_Value'] = summary['Total_Neg_Value'].abs()

# ======================================================
# 2Ô∏è‚É£ Add TOTAL per Store and Grand Total
# ======================================================
store_totals = (
    summary.groupby('STORE_NAME', as_index=False)
           .agg(
               Total_Neg_Value=('Total_Neg_Value','sum'),
               Total_Count=('Total_Count','sum'),
               Abs_Neg_Value=('Abs_Neg_Value','sum')
           )
)
store_totals['Sale_Type'] = 'ALL'
store_totals['CAP_CUSTOMER_CODE'] = '‚Äî'
summary_total = pd.concat([summary, store_totals], ignore_index=True)

grand_total = pd.DataFrame({
    'STORE_NAME': ['TOTAL'],
    'Sale_Type': ['ALL'],
    'CAP_CUSTOMER_CODE': ['‚Äî'],
    'Total_Neg_Value': [summary['Total_Neg_Value'].sum()],
    'Total_Count': [summary['Total_Count'].sum()],
    'Abs_Neg_Value': [summary['Abs_Neg_Value'].sum()]
})
summary_total = pd.concat([summary_total, grand_total], ignore_index=True)

# ======================================================
# 3Ô∏è‚É£ Interactive Filters
# ======================================================
branch_dd = widgets.Dropdown(
    options=['All'] + sorted(d['STORE_NAME'].unique().tolist()),
    value='All',
    description='Branch:',
    layout=widgets.Layout(width='260px')
)

stype_dd = widgets.Dropdown(
    options=['All','General sales','On_account sales'],
    value='All',
    description='Type:',
    layout=widgets.Layout(width='240px')
)

metric_dd = widgets.Dropdown(
    options=[('Value (KSh)','value'), ('Count','count')],
    value='value',
    description='Rank by:',
    layout=widgets.Layout(width='220px')
)

topn_in = widgets.BoundedIntText(
    value=50, min=5, max=1000, step=5,
    description='Top N:',
    layout=widgets.Layout(width='160px')
)

run_btn = widgets.Button(
    description='üîç Update',
    button_style='primary',
    layout=widgets.Layout(width='120px', height='34px')
)

controls = widgets.HBox([branch_dd, stype_dd, metric_dd, topn_in, run_btn])
out = widgets.Output()
display(controls, out)

# ======================================================
# 4Ô∏è‚É£ Display Logic
# ======================================================
def render(_=None):
    with out:
        out.clear_output()

        dfv = summary_total.copy()

        # Filter by branch
        if branch_dd.value != 'All':
            dfv = dfv[dfv['STORE_NAME'] == branch_dd.value]

        # Filter by sale type
        if stype_dd.value != 'All':
            dfv = dfv[dfv['Sale_Type'] == stype_dd.value]

        # Sort by metric
        if metric_dd.value == 'value':
            dfv = dfv.sort_values('Abs_Neg_Value', ascending=False)
        else:
            dfv = dfv.sort_values('Total_Count', ascending=False)

        # Take top N (but always include TOTAL)
        dfv = pd.concat([dfv.head(topn_in.value), dfv[dfv['STORE_NAME'] == 'TOTAL']]).drop_duplicates()
        dfv = dfv.reset_index(drop=True)
        dfv.insert(0, '#', range(1, len(dfv)+1))

        # Format
        dfv['Total_Neg_Value'] = dfv['Total_Neg_Value'].map(lambda x: f"{x:,.2f}")
        dfv['Total_Count'] = dfv['Total_Count'].map('{:,.0f}'.format)
        dfv['Abs_Neg_Value'] = dfv['Abs_Neg_Value'].map(lambda x: f"{x:,.2f}")

        # Final columns
        disp = dfv[['#','STORE_NAME','Sale_Type','CAP_CUSTOMER_CODE','Total_Neg_Value','Total_Count']]
        title = "üßæ Negative Receipts Summary by Branch & CAP_CUSTOMER_CODE"
        sub = f"(Ranked by {'Value (Absolute KSh)' if metric_dd.value=='value' else 'Count'})"
        print(title, sub)
        display(disp)

run_btn.on_click(render)
render()

"""##Branch Refunds Overview"""
